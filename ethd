#!/usr/bin/env bash
set -Eeuo pipefail

__project_name="Eth Docker"
__app_name="Ethereum node"
__sample_service="consensus"
__docker_exe="docker"
__compose_exe="docker compose"
__compose_upgraded=0


dodocker() {
  $__docker_sudo $__docker_exe "$@"
}


docompose() {
# I want word splitting here
# shellcheck disable=SC2086
  $__docker_sudo $__compose_exe "$@"
}


determine_distro() {
# Determine OS platform
  __uname=$(uname | tr "[:upper:]" "[:lower:]")
# If Linux, try to determine specific distribution
  if [ "$__uname" == "linux" ]; then
# If available, use LSB to identify distribution
    if [ -n "$(command -v lsb_release 2>/dev/null)" ]; then
      __distro=$(lsb_release -i | cut -d: -f2 | sed s/'^\t'//)
# Otherwise, use release info file
    else
      __distro=$(find /etc -maxdepth 1 -type f -name '[A-Za-z]*[_-][rv]e[lr]*' \
        | grep -v "lsb" | cut -d'/' -f3 | cut -d'-' -f1 | cut -d'_' -f1)
    fi
  else
    __distro=""
  fi
# For everything else (or if above failed), just use generic identifier
  [ "$__distro" == "" ] && __distro=$__uname
  unset __uname
  __distro=$(echo "$__distro" | tr "[:upper:]" "[:lower:]")

  if [[ "$__distro" = "ubuntu" ]]; then
    if ! dpkg-query -W -f='${Status}' lsb-release 2>/dev/null | grep -q "ok installed"; then
      ${__auto_sudo} apt-get update && ${__auto_sudo} apt-get -y install lsb-release
    fi
    __os_major_version=$(lsb_release -r | cut -d: -f2 | sed s/'^\t'// | cut -d. -f1)
  elif [[ "$__distro" =~ "debian" ]]; then
    if ! dpkg-query -W -f='${Status}' lsb-release 2>/dev/null | grep -q "ok installed"; then
      ${__auto_sudo} apt-get update && ${__auto_sudo} apt-get -y install lsb-release
    fi
    __os_major_version=$(lsb_release -r | cut -f2)
  fi
}


handle_docker_sudo() {
  set +e
  if [[ "$__distro" =~ "debian" || "$__distro" = "ubuntu" ]]; then
    systemctl status docker >/dev/null
    result=$?
    if [ ! "${result}" -eq 0 ]; then
      echo "The Docker daemon is not running. Please check Docker installation."
      echo "\"sudo systemctl status docker\" and \"sudo journalctl -fu docker\" will be helpful."
      echo "Aborting."
      exit 1
    fi
  fi
  set -e

  __docker_version=$(docker --version | awk '{ gsub(/,/, "", $3); print $3 }')
  __docker_major_version=$(docker --version | awk '{ split($3, version, "."); print version[1]; }')
  if [ "${__docker_major_version}" -lt 23 ]; then
    __old_docker=1
    echo "Docker ${__docker_version} detected"
  else
    __old_docker=0
  fi
  __docker_sudo=""
  if ! docker images >/dev/null 2>&1; then
    echo "Will use sudo to access Docker"
    __docker_sudo="sudo"
  fi
}


handle_root() {
  if [ "${EUID}" -eq 0 ]; then
    __as_owner="sudo -u ${OWNER}"
    __auto_sudo=""
  else
    __as_owner=""
    __auto_sudo="sudo"
  fi
}


upgrade_compose() {
  if ! type -P docker-compose >/dev/null 2>&1; then
    echo "Docker Compose has already been updated to V2"
    return
  fi
  echo "Updating Docker Compose to V2"
  if [[ "$__distro" = "ubuntu" ]]; then
    if [ "${__os_major_version}" -lt 22 ]; then
      echo "${__project_name} cannot update Docker Compose on Ubuntu ${__os_major_version}."
      echo "Consider upgrading to 22.04 and then 24.04."
      exit 1
    fi
    ${__auto_sudo} apt-get update
    ${__auto_sudo} apt-get install -y docker-compose-v2 docker-buildx
    echo "Installed docker-compose-v2"
    __old_compose=0
    __compose_upgraded=1
    if dpkg-query -W -f='${Status}' docker.io 2>/dev/null | grep -q "ok installed"; then
        ${__auto_sudo} apt-mark manual docker.io
    elif dpkg-query -W -f='${Status}' docker-ce 2>/dev/null | grep -q "ok installed"; then
        ${__auto_sudo} apt-mark manual docker-ce
    fi
    ${__auto_sudo} apt-get remove -y docker-compose
    echo "Removed docker-compose"
  elif [[ "$__distro" =~ "debian" ]]; then
    ${__auto_sudo} apt-get update && ${__auto_sudo} apt-get -y install ca-certificates curl gnupg
    if [ "${__os_major_version}" -lt 11 ]; then
        echo "${__project_name} cannot update Docker Compose on Debian ${__os_major_version}."
        echo "Consider upgrading to 11 and then 12."
        exit 1
    fi
    ${__auto_sudo} mkdir -p /etc/apt/keyrings
    ${__auto_sudo} curl -fsSL https://download.docker.com/linux/debian/gpg | ${__auto_sudo} gpg --dearmor --yes \
    -o /etc/apt/keyrings/docker.gpg
    ${__auto_sudo} echo \
        "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
        https://download.docker.com/linux/debian $(lsb_release -cs) stable" \
        | ${__auto_sudo} tee /etc/apt/sources.list.d/docker.list > /dev/null
    ${__auto_sudo} apt-get update
    ${__auto_sudo} apt-get install -y docker-compose-plugin docker-buildx-plugin
    echo "Installed docker-compose-plugin"
    __old_compose=0
    __compose_upgraded=1
    if dpkg-query -W -f='${Status}' docker.io 2>/dev/null | grep -q "ok installed"; then
        ${__auto_sudo} apt-mark manual docker.io
    elif dpkg-query -W -f='${Status}' docker-ce 2>/dev/null | grep -q "ok installed"; then
        ${__auto_sudo} apt-mark manual docker-ce
    fi
    ${__auto_sudo} apt-get remove -y docker-compose
    echo "Removed docker-compose"
  else
    echo "${__project_name} does not know how to update Docker Compose on $__distro"
  fi
}


check_compose_version() {
# Check for Compose V2 (docker compose) vs Compose V1 (docker-compose)
  if docker compose version >/dev/null 2>&1; then
    __compose_version=$($__docker_sudo docker compose version | sed -n -E -e "s/.*version [v]?([0-9.-]*).*/\1/ip")
    __compose_major=${__compose_version%%.*}
   if [[ "${__compose_major}" -eq 1 ]]; then
     __old_compose=1
   else
     __old_compose=0
   fi
  else
    __old_compose=1
    __compose_version=$($__docker_sudo docker-compose --version | sed -n -E -e "s/.*version [v]?([0-9.-]*).*/\1/ip")
  fi
  if [ "${__old_compose}" -eq 1 ]; then
    if [ -n "${ETHDSECUNDO-}" ]  || [ ! "${__command}" = "update" ]; then # Don't run this twice
      echo
      echo "You are using docker-compose ${__compose_version}, which is unsupported by Docker, Inc."
      echo "${__project_name} only supports Compose V2."
      echo
      echo "It is recommended that you replace Compose V1 with Compose V2."
      while true; do
        read -rp "Do you want to update Docker Compose to V2? (yes/no) " yn
        case $yn in
          [Nn]* ) echo "Please be sure to update Docker Compose yourself!"; break;;
           * ) upgrade_compose; break;;
        esac
      done
    fi
  fi
}


prep_conffiles() {
# Create custom-prom.yml if it doesn't exist
  if [ ! -f "./prometheus/custom-prom.yml" ]; then
    ${__as_owner} touch "./prometheus/custom-prom.yml"
  fi
# Move ssv-config.yaml
  if [ -f "./ssv-config.yaml" ]; then
    ${__as_owner} mv ./ssv-config.yaml ssv-config/config.yaml
  fi
# Create config.yaml if it doesn't exist
  if [ ! -f "ssv-config/config.yaml" ]; then
    ${__as_owner} cp ssv-config/config-sample.yaml ssv-config/config.yaml
  fi
  if [ ! -f "ssv-config/dkg-config.yaml" ]; then
    ${__as_owner} cp ssv-config/dkg-config-sample.yaml ssv-config/dkg-config.yaml
  fi
# Make sure local user owns the dkg output dir and everything in it
  if find .eth/dkg_output \! -user "${OWNER}" -o \! -group "${OWNER_GROUP}" -o \! -perm 755 | grep -q .; then
    ${__auto_sudo} chown -R "${OWNER}:${OWNER_GROUP}" .eth/dkg_output
    ${__auto_sudo} chmod -R 755 .eth/dkg_output
  fi
# Create ext-network.yml if it doesn't exist
  if [ ! -f "ext-network.yml" ]; then
    ${__as_owner} cp ext-network.yml.sample ext-network.yml
  fi
}


check_for_snap() {
  if [[ "$__distro" = "ubuntu" && -n "$(command -v snap)" ]] && snap list 2>/dev/null | grep -qw 'docker'; then
    echo
    echo "WARNING! Snap Docker package detected. This WILL result in issues."
    echo "Removing the package will delete volumes and require a resync."
    echo
    echo "Doing so is still highly recommended however."
    echo
    echo "The exact steps depend a little on whether there already is an apt version of Docker installed as well,"
    echo "but in a nutshell \"$__me stop\" followed by \"sudo snap remove --purge docker\" followed by a reboot,"
    echo "and as needed install docker-ce or docker.io with apt."
    echo
    echo "Do join us on EthStaker Discord to work through this issue."
    echo
    echo "Aborting, this is not safe"
    exit 1
  fi
}


install-bash-completions() {
  if [[ "$OSTYPE" == "darwin"* ]]; then
    echo "Skipping installation of tab completions (not supported on macOS)"
  else
    if [ ! -f "$(pkg-config --variable=completionsdir bash-completion)/ethd" ]; then
      echo "Installing bash completions for ethd"
      ${__auto_sudo} ln -s "$(dirname "$(realpath "${BASH_SOURCE[0]}")")/bash-completion" \
      "$(pkg-config --variable=completionsdir bash-completion)/ethd" || true
    fi
  fi
}


install() {

  if [[ "$__distro" = "ubuntu" ]]; then
    ${__auto_sudo} apt-get update
    ${__auto_sudo} apt-get install -y ca-certificates curl gnupg whiptail chrony pkg-config
    echo
    echo
    if [ -z "$(command -v docker)" ]; then
      if [ "${__os_major_version}" -lt 22 ]; then
        echo "${__project_name} cannot install Docker on Ubuntu ${__os_major_version}."
        echo "Consider upgrading to 22.04 and then 24.04."
        exit 1
      fi
      read -rp "This will attempt to install Docker and make your user part of the docker group. Do you wish to \
continue? (no/yes) " yn
      case $yn in
        [Yy]* ) ;;
        * ) echo "Aborting, no changes made"; return 0;;
      esac
      ${__auto_sudo} mkdir -p /etc/apt/keyrings
      ${__auto_sudo} curl -fsSL https://download.docker.com/linux/ubuntu/gpg | ${__auto_sudo} gpg --dearmor \
        --yes -o /etc/apt/keyrings/docker.gpg
      ${__auto_sudo} echo \
        "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
        https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" \
        | ${__auto_sudo} tee /etc/apt/sources.list.d/docker.list > /dev/null
      ${__auto_sudo} apt-get update
      ${__auto_sudo} apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin \
        docker-buildx-plugin
      echo "Installed docker-ce and docker-compose-plugin"
    else
      echo "Docker is already installed"
    fi
    __groups=$(${__as_owner} groups)
    if [[ ! "$__groups" =~ "docker" ]]; then
      echo "Making your user part of the docker group"
      ${__auto_sudo} usermod -aG docker "${OWNER}"
      echo "Please run newgrp docker or log out and back in"
    else
      echo "Your user is already part of the docker group"
    fi
  elif [[ "$__distro" =~ "debian" ]]; then
    ${__auto_sudo} apt-get update
    ${__auto_sudo} apt-get -y install ca-certificates curl gnupg whiptail chrony pkg-config
    echo
    echo
    if [ -z "$(command -v docker)" ]; then
      if [ "${__os_major_version}" -lt 11 ]; then
        echo "${__project_name} cannot install Docker on Debian ${__os_major_version}."
        echo "Consider upgrading to 11 and then 12."
        exit 1
      fi
      read -rp "This will attempt to install Docker and make your user part of the docker group. Do you wish to \
continue? (no/yes) " yn
      case $yn in
        [Yy]* ) ;;
        * ) echo "Aborting, no changes made"; return 0;;
      esac
      ${__auto_sudo} mkdir -p /etc/apt/keyrings
      ${__auto_sudo} curl -fsSL https://download.docker.com/linux/debian/gpg | ${__auto_sudo} gpg --dearmor \
        --yes -o /etc/apt/keyrings/docker.gpg
      ${__auto_sudo} echo \
        "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
        https://download.docker.com/linux/debian $(lsb_release -cs) stable" \
        | ${__auto_sudo} tee /etc/apt/sources.list.d/docker.list > /dev/null
      ${__auto_sudo} apt-get update
      ${__auto_sudo} apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin \
        docker-buildx-plugin
      echo "Installed docker-ce and docker-compose-plugin"
    else
      echo "Docker is already installed"
    fi
    __groups=$(${__as_owner} groups)
    if [[ ! "$__groups" =~ "docker" ]]; then
      echo "Making your user part of the docker group"
      ${__auto_sudo} usermod -aG docker "${OWNER}"
      echo "Please run newgrp docker or log out and back in"
    else
      echo "Your user is already part of the docker group"
    fi
  else
    echo "${__project_name} does not know how to install Docker on $__distro"
  fi

  if ! [[ "$__distro" = "ubuntu" ]] || [[ "$__distro" =~ "debian" ]]; then
    return 0
  fi

  # We only get here on Ubuntu or Debian
  install-bash-completions
  __install_base=$(basename "$(dirname "$(realpath "${BASH_SOURCE[0]}")")")
  if [ "${__install_base}" = "eth-docker" ]; then
    read -rp "Do you want to be able to call 'ethd' from anywhere? (yes/no) " yn
    case $yn in
      [Nn]* ) return 0;;
      * ) ;;
    esac
    if grep -q "alias ethd" ~/.profile; then
      sed -i'.original' -e "/alias ethd/d" ~/.profile
    fi
    echo "alias ethd=$(realpath "${BASH_SOURCE[0]}")" >>~/.profile
    if grep -q "cat.*\.motd" ~/.profile; then
      sed -i'.original' -e "/cat.*\.motd/d" ~/.profile
    fi
    echo "cat $(dirname "$(realpath "${BASH_SOURCE[0]}")")/.motd" >>~/.profile
    echo "Go ahead and 'source ~/.profile' or log out and back in."
    echo "After that, you can use the command 'ethd'."
  fi

  return 0
}


__get_docker_free_space() { # set __free_space to what's available to Docker
  if [[ "$OSTYPE" == "darwin"* ]]; then # macOS doesn't expose docker root dir to the OS
    __free_space=$(dodocker run --rm -v macos-space-check:/dummy busybox df -P /dummy | awk '/[0-9]%/{print $(NF-2)}')
  else
    __docker_dir=$(dodocker system info --format '{{.DockerRootDir}}')
    __free_space=$(df -P "${__docker_dir}" | awk '/[0-9]%/{print $(NF-2)}')
  fi

  re='^[0-9]+$'
  if ! [[ "${__free_space}" =~ $re ]] ; then
    echo "Unable to determine free disk space. This is likely a bug."
    if [[ "$OSTYPE" == "darwin"* ]]; then
      echo "df reports $(dodocker run --rm -v macos-space-check:/dummy busybox df -P /dummy) and __free_space is ${__free_space}"
    else
      echo "df reports $(df -P "${__docker_dir}") and __free_space is ${__free_space}"
    fi
    exit 70
  fi
}


__display_docker_dir() {
  if [[ "$OSTYPE" == "darwin"* ]]; then # macOS doesn't expose docker root dir to the OS
    echo "Here's total and used space on Docker's virtual volume"
    dodocker run --rm -v macos-space-check:/dummy busybox df -h /dummy
  else
    echo "Here's total and used space on ${__docker_dir}"
    df -h "${__docker_dir}"
  fi
}


__display_docker_volumes() {
  echo
  if [ -z "$(dodocker volume ls -q -f "name=^$(basename "$(realpath .)")_[^_]+")" ]; then
    echo "There are no Docker volumes for this copy of ${__project_name}"
    echo
  else
    echo "Here are the Docker volumes used by this copy of ${__project_name} and their space usage:"
    dodocker system df -v | grep -A 50 "VOLUME NAME" | grep "^$(basename "$(dirname "$(realpath "${BASH_SOURCE[0]}")")")"
    echo
    echo "If your Consensus Layer client takes more than 300 GiB, you can resync it with"
    echo "\"${__me} resync-consensus\"."
    echo
  fi
  echo "If there is some mystery space being taken up, try \"sudo ncdu /\"."
  echo
}


space() {
  __get_docker_free_space
  echo
  if [[ "$OSTYPE" == "darwin"* ]]; then # macOS doesn't expose docker root dir to the OS
    echo "You have $(( __free_space / 1024 / 1024 )) GiB free for Docker volumes"
  else
    echo "You have $(( __free_space / 1024 / 1024 )) GiB free on ${__docker_dir}"
  fi
  echo
  __display_docker_dir
  __display_docker_volumes
}


# Warn user if space is low, so they can prune
check_disk_space() {
  __get_docker_free_space

  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  var="AUTOPRUNE_NM"
  auto_prune=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  var="NETWORK"
  NETWORK=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  if [ "${NETWORK}" = "mainnet" ] || [ "${NETWORK}" = "gnosis" ]; then
    __min_free=314572800
    __min_gib=300
    __safe_prune=250
  else
    __min_free=31457280
    __min_gib=30
    __safe_prune=25
  fi

# Literal match intended
# shellcheck disable=SC2076
  if [[ "${value}" =~ "nethermind.yml" ]] && [[ "${__free_space}" -lt "${__min_free}" ]]; then
    echo
    echo "You are running Nethermind and have less than ${__min_gib} GiB of free disk space."
    if [ "${auto_prune}" = true ]; then
      echo "It should currently be auto-pruning, check logs with \"$__me logs -f --tail 500 execution | grep \
Full\". Free space:"
    else
      echo "If the below reads above ${__safe_prune} GiB free, prune it with \"$__me prune-nethermind\""
    fi
    echo
    __display_docker_dir
    __display_docker_volumes
  elif [[ "${value}" =~ "geth.yml" ]] && [[ "${__free_space}" -lt 104857600 ]]; then
    echo
    echo "You are running Geth and have less than 100 GiB of free disk space."
    echo "You may resync from scratch to use PBSS and slow on-disk DB growth, with \"$__me resync-execution\"."
    echo
    __display_docker_dir
    __display_docker_volumes
  elif [[ "${value}" =~ "besu.yml" ]] && [[ "${__free_space}" -lt 52428800 ]]; then
    echo
    echo "You are running Besu and have less than 50 GiB of free disk space."
    echo
    echo "If this is a long-running Besu, you may prune trie-logs with \"$__me prune-besu\"."
    __display_docker_volumes
    echo
  elif [[ "${__free_space}" -lt 52428800 ]]; then
    echo
    echo "You have less than 50 GiB of free disk space:"
    echo
    __display_docker_dir
    echo
    echo "Pruning does not appear an option for your client mix."
    echo "If total space is less than 1.8 TiB, consider cloning to a larger drive."
    __display_docker_volumes
  fi
}


source_build() {
# Check whether there's a source-built client and if so, force it with --no-cache
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  case "${value}" in
    *deposit-cli.yml* )
      docompose --profile tools build --pull --no-cache deposit-cli-new
      ;;
  esac
  case "${value}" in
    *mev-boost.yml* )
      var="MEV_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache mev-boost
      fi
      ;;
  esac
  case "${value}" in
    *reth.yml* )
      var="RETH_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
    *geth.yml* )
      var="GETH_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
    *besu.yml* )
      var="BESU_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
    *nethermind.yml* )
      var="NM_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
    *erigon.yml* )
      var="ERIGON_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
    *nimbus-el.yml* )
      var="NIMEL_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache execution
      fi
      ;;
  esac
  case "${value}" in
    *lighthouse.yml* | *lighthouse-cl-only.yml* )
      var="LH_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
    *teku.yml* | *teku-allin1.yml* | *teku-cl-only.yml* )
      var="TEKU_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
    *lodestar.yml* | *lodestar-cl-only.yml* )
      var="LS_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
    *nimbus.yml* | *nimbus-allin1.yml* | *nimbus-cl-only.yml* )
      var="NIM_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
    *prysm.yml* | *prysm-cl-only.yml* )
      var="PRYSM_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
    *grandine.yml* | *grandine-allin1.yml* | *grandine-cl-only.yml* )
      var="GRANDINE_DOCKERFILE"
      build=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
      if [ "${build}" = "Dockerfile.source" ]; then
          docompose build --pull --no-cache consensus
      fi
      ;;
  esac
}


migrate_compose_file() {
# When this gets called $var is COMPOSE_FILE and $value is what is set in .env for it
# Some files have been renamed and others removed altogether
  FROM_YML=( )
  TO_YML=( )

  IFS=":"
  set -o noglob
# Globbing is off
# shellcheck disable=SC2206
  __ymlarray=($value) # split+glob with glob disabled, and split using : as delimiter
  set +o noglob
# Unset restores default
  unset IFS

  value=""
  for n in "${!__ymlarray[@]}"; do
    __ymlfile="${__ymlarray[n]}"
    for index in "${!FROM_YML[@]}"; do
      if [ "${FROM_YML[index]}" = "${__ymlfile}" ]; then
        __ymlfile=${TO_YML[index]}
        break
      fi
    done
    if [ -n "${__ymlfile}" ]; then
      if [ -z "${value}" ]; then
        value="${__ymlfile}"
      else
        value="${value}:${__ymlfile}"
      fi
    fi
  done
}


ssv_switch() {
  echo "Detected legacy SSV Node. Migrating config to new testnet."
  echo
  echo "Stopping SSV Node container"
  __node=$(dodocker ps --format '{{.Names}}' | grep 'ssv2-node')
  dodocker stop "${__node}" && dodocker rm -f "${__node}"
  dodocker volume rm "$(dodocker volume ls -q | grep "$(basename "$(realpath .)")"_ssv2-data)"
  echo
  echo "SSV Node stopped and database deleted."
  echo
  cp blox-ssv-config.yaml blox-ssv-config.yaml.bak
  cp blox-ssv-config.yaml ssv-config/config.yaml
  rm blox-ssv-config.yaml
  echo "Backup copy blox-ssv-config.yaml.bak created"
  echo "Making changes to ssv-config/config.yaml"
  var="NETWORK"
  NETWORK=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  sed -i'.original' 's/blox-ssv2.yml/ssv.yml/' "${ENV_FILE}".source
  if ! grep -q "LogFilePath:" ssv-config/config.yaml; then
# macOS-isms: Newline for sed add
    sed -i'.original' '/global:/a\
  LogFilePath: /tmp/ssv/debug.log
' ssv-config/config.yaml
  fi
  if ! grep -q "MetricsAPIPort:" ssv-config/config.yaml; then
    sed -i'.original' '$a\
MetricsAPIPort: 15000
' ssv-config/config.yaml
  fi
  if ! grep -q "ssv:" ssv-config/config.yaml; then
    sed -i'.original' '/^  Network:/d' ssv-config/config.yaml # Remove old eth2 Network line if present
    sed -i'.original' '$a\
 ssv:
 ' ssv-config/config.yaml
    if [ "${NETWORK}" = "holesky" ]; then
      sed -i'.original' '$a\
  Network: holesky
' ssv-config/config.yaml
    elif [ "${NETWORK}" = "mainnet" ]; then
      sed -i'.original' '$a\
  Network: mainnet
' ssv-config/config.yaml
    else
      echo "${NETWORK} is not something that works with SSV."
      echo "Please fix this manually before running $__me update again."
      echo "Aborting."
      exit 1
    fi
  fi
  rm ssv-config/config.yaml.original
}


delete_reth() {
# Check for Reth
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "reth.yml" ]]; then
    return 0
  fi

# Check Reth version, only continue if not on alpha
  var="RETH_DOCKER_TAG"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ "${value}" =~ "alpha" ]]; then
    return 0
  fi

  if [ -z "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]reth-el-data")" ]; then # No Reth volume
    return 0
  fi

# Check Reth db version
  __db_version="$(dodocker run --rm -v "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]reth-el-data")":"/var/lib/reth" \
      alpine:3 cat /var/lib/reth/db/database.version)"
  if [ "${__db_version}" -ne "1" ]; then
    return 0
  fi

  echo "Detected Reth. For Reth beta, it will need to be re-synced from scratch."
  echo
  if [ "${__non_interactive:-0}" -eq 0 ]; then
    while true; do
      read -rp "WARNING - About to delete the Reth database. Do you wish to continue? (Y/n) " yn
      case $yn in
        [Nn]o | [Nn]  ) echo "No changes made"; return 0;;
        * ) break;;
      esac
    done
  fi

  echo "Stopping Reth container"
  docompose stop execution && docompose rm -f execution
  dodocker volume rm "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]reth-el-data")"
  echo
  echo "Reth stopped and database deleted."
  echo
}


delete_erigon() {
# Check for Erigon
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "erigon.yml" ]]; then
    return 0
  fi

# Check Erigon version, only continue if v3
  var="ERIGON_DOCKER_TAG"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# I do mean to match literally
# shellcheck disable=SC2076
#  if [[ ! ("${value}" =~ "v3" ||  "${value}" = "latest" ||  "${value}" = "stable") ]]; then # No stable yet
  if [[ ! ("${value}" =~ "v3" ||  "${value}" = "latest") ]]; then
    return 0
  fi

  if [ -z "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]erigon-el-data")" ]; then # No Erigon volume
    return 0
  fi

# Detect Erigon v3 by directory caplin/latest
  __erigon_v3=$(dodocker run --rm -v "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]erigon-el-data")":"/var/lib/erigon" \
      alpine:3 sh -c 'if [ -d "/var/lib/erigon/caplin/latest" ]; then echo true; else echo false; fi')
  if [ "$__erigon_v3" = "true" ]; then
    return 0
  fi

  echo "Detected Erigon. For Erigon v3, it will need to be re-synced from scratch."
  echo
  while true; do
    read -rp "WARNING - About to delete the Erigon database. Do you wish to continue? (Y/n) " yn
    case $yn in
      [Nn]o | [Nn]  ) echo "Aborting, no changes made"; exit 130;;
      * ) break;;
    esac
  done

  echo "Stopping Erigon container"
  docompose stop execution && docompose rm -f execution
  dodocker volume rm "$(dodocker volume ls -q -f "name=$(basename "$(realpath .)")[_-]erigon-el-data")"
  echo
  echo "Erigon stopped and database deleted."
  echo
}


upgrade_postgres() {
# Check for web3signer
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "web3signer.yml" ]]; then
    return 0
  fi

  __source_vol="$(basename "$(pwd)")_web3signer-slashing-data"
  if [ -z "$(dodocker volume ls -q -f "name=${__source_vol}")" ]; then
    return 0
  fi

  __target_pg=16
  __during_postgres=1

  __source_pg="$(dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/data" \
    alpine:3 cat /var/lib/postgresql/data/PG_VERSION)"

  if [ "${__source_pg}" -lt "${__target_pg}" ]; then
    echo "Web3signer is using PostgreSQL ${__source_pg}. The current version is PostgreSQL ${__target_pg}."
    echo
    if [ "${__non_interactive:-0}" -eq 0 ]; then
      while true; do
        read -rp "Would you like to migrate to PostgreSQL ${__target_pg}? (Y/n) " yn
        case $yn in
          [Nn]o | [Nn]  ) echo "Keeping PostgreSQL at version ${__source_pg}"; return 0;;
          * ) break;;
        esac
      done
    fi
  else
    return 0
  fi

  __source_size="$(dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/data" \
    alpine:3 du -s /var/lib/postgresql/data/ | awk '{print $1}')"

  re='^[0-9]+$'
  if ! [[ "${__source_size}" =~ $re ]] ; then
    echo "Unable to determine database size. This is likely a bug."
    echo "__source_size is ${__source_size}"
    return 70
  fi

  __get_docker_free_space

  if [[ "${__free_space}" -lt $(( (__source_size * 2) + 10485760 )) ]]; then
    echo
    echo "You don't have enough free space to migrate the database."
    echo "It is $(( __source_size / 1024 / 1024 )) GiB in size and you need twice as much free again."
    echo
    __display_docker_dir
    echo
    return
  fi

  __migrated_vol="$(basename "$(pwd)")_web3signer-slashing-data-pg${__target_pg}-migrated"
  __backup_vol="$(basename "$(pwd)")_web3signer-slashing-data-pg${__source_pg}-backup"

  echo "Stopping Web3signer"
  docompose stop web3signer && docompose rm -f web3signer
  echo "Stopping PostgreSQL"
  docompose stop postgres && docompose rm -f postgres

  echo
  echo "Migrating database from PostgreSQL ${__source_pg} to PostgreSQL ${__target_pg}"
  echo "If this step fails, the Web3signer slashing protection database is no longer protecting you."
  echo "In failure case, do not start Web3signer again, instead seek help on Ethstaker Discord."
  echo

  dodocker pull "pats22/postgres-upgrade:${__source_pg}-to-${__target_pg}"
  dodocker volume create "${__migrated_vol}"
  dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/${__source_pg}/data" \
   -v "${__migrated_vol}":"/var/lib/postgresql/${__target_pg}/data" \
   "pats22/postgres-upgrade:${__source_pg}-to-${__target_pg}"
# Adjust ownership. We use 70; postgres-upgrade creates it with 999
  dodocker run --rm -v "${__migrated_vol}":"/var/lib/postgres" \
    alpine:3 chown -R 70:70 /var/lib/postgres
# Conversion can leave us with a pg_hba.conf that does not allow connections
  dodocker run --rm -v "${__migrated_vol}":"/var/lib/postgres" \
    alpine:3 sh -c 'grep -qxE "host\s+all\s+all\s+all\s+scram-sha-256" /var/lib/postgres/pg_hba.conf \
    || echo "host    all             all             all                     scram-sha-256" \
    >> /var/lib/postgres/pg_hba.conf'

  echo
  echo "Migration complete, copying data in web3signer-slashing-data volume to backup"
  dodocker volume create "${__backup_vol}"
  dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/data" \
    -v "${__backup_vol}":"/var/lib/postgresql/${__source_pg}/data" \
    alpine:3 cp -a /var/lib/postgresql/data/. "/var/lib/postgresql/${__source_pg}/data/"

  __during_migrate=1
  echo "Moving migrated data to web3signer-slashing-data volume"
  dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/data" \
    alpine:3 rm -rf /var/lib/postgresql/data/*
  dodocker run --rm -v "${__source_vol}":"/var/lib/postgresql/data" \
    -v "${__migrated_vol}":"/var/lib/postgresql/${__target_pg}/data" \
    alpine:3 cp -a "/var/lib/postgresql/${__target_pg}/data/." /var/lib/postgresql/data/

  __migrated=1
  dodocker volume remove "${__migrated_vol}"

  echo
  echo "Adjusting PostgreSQL Docker tag"
  if [ ! -f "${ENV_FILE}.source" ]; then # update() didn't migrate env, let's make sure .env.source exists
    cp "${ENV_FILE}" "${ENV_FILE}.source"
  fi
  var="PG_DOCKER_TAG"
# This gets used, but shellcheck doesn't recognize that
# shellcheck disable=SC2034
  PG_DOCKER_TAG=${__target_pg}-bookworm # To bookworm to avoid collation errors - also a faster PostgreSQL
  set_value_in_env
  echo "Web3signer has been stopped. You'll need to run \"$__me up\" to start it again."
  echo
  echo "A copy of your old slashing protection database is in the Docker volume ${__backup_vol}."
  echo "Confirm that everything works, and then delete it with \"docker volume rm ${__backup_vol}\"."
  __during_postgres=0
}


__lookup_cf_zone() { # Migrates traefik-cf setup to use Zone ID
  __compose_ymls=$(sed -n -e "s/^COMPOSE_FILE=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
  __dns_token=$(sed -n -e "s/^CF_DNS_API_TOKEN=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
  __zone_token=$(sed -n -e "s/^CF_ZONE_API_TOKEN=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
  __domain=$(sed -n -e "s/^DOMAIN=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
  if [[ ! $__compose_ymls =~ traefik-cf.yml ]]; then
    value=""
    return
  elif [[ -n $__dns_token ]]; then
    if [[ -n $__zone_token ]]; then
      __token=$__zone_token
    else
      __token=$__dns_token
    fi
    set +e
    value=$(docompose run --rm curl-jq sh -c \
      "curl -s \"https://api.cloudflare.com/client/v4/zones?name=${__domain}\" -H \"Authorization: Bearer ${__token}\" \
      -H \"Content-Type: application/json\" | jq -r '.result[0].id'" | tail -n 1)
    __code=$?
    if [[ "$__code" -ne 0 ]]; then
      value=""
      return
    fi
    __success=$(docompose run --rm curl-jq sh -c \
      "curl -s \"https://api.cloudflare.com/client/v4/zones?name=${__domain}\" -H \"Authorization: Bearer ${__token}\" \
      -H \"Content-Type: application/json\" | jq -r '.success'" | tail -n 1)
    set -e
    if [ "${__success}" = "true" ]; then
      return
    else
      value=""
      return
    fi
  else
    value=""
    return
  fi
}


envmigrate() {
  if [ ! -f "${ENV_FILE}" ]; then
    return 0
  fi

  ALL_VARS=( COMPOSE_FILE FEE_RECIPIENT EL_NODE EL_OBOL_NODE GRAFFITI DEFAULT_GRAFFITI NETWORK MEV_BOOST MEV_RELAYS MEV_MIN_BID \
    MEV_NODE CL_MAX_PEER_COUNT CL_MIN_PEER_COUNT EL_MAX_PEER_COUNT EL_MIN_PEER_COUNT DOMAIN ACME_EMAIL ANCIENT_DIR \
    AUTOPRUNE_NM LOGS_LABEL CF_DNS_API_TOKEN CF_ZONE_API_TOKEN CF_ZONE_ID AWS_PROFILE AWS_HOSTED_ZONE_ID \
    GRAFANA_HOST SIREN_HOST DISTRIBUTED BESU_HEAP TEKU_HEAP PROM_HOST HOST_IP SHARE_IP PRYSM_HOST EE_HOST \
    EL_HOST EL_LB EL_WS_HOST EL_WS_LB CL_HOST CL_LB VC_HOST DDNS_SUBDOMAIN IPV6 DDNS_PROXY RAPID_SYNC_URL \
    CL_NODE CL_OBOL_NODE BEACON_STATS_API BEACON_STATS_MACHINE EL_P2P_PORT CL_P2P_PORT WEB3SIGNER PRYSM_PORT DOPPELGANGER \
    PRYSM_UDP_PORT CL_QUIC_PORT GRAFANA_PORT SIREN_PORT PROMETHEUS_PORT KEY_API_PORT TRAEFIK_WEB_PORT \
    TRAEFIK_WEB_HTTP_PORT CL_REST_PORT EL_RPC_PORT EL_WS_PORT EE_PORT ERIGON_TORRENT_PORT LOG_LEVEL JWT_SECRET \
    EL_EXTRAS CL_EXTRAS VC_EXTRAS ARCHIVE_NODE SSV_P2P_PORT SSV_P2P_PORT_UDP OBOL_P2P_PORT ERIGON_P2P_PORT_2 \
    ERIGON_P2P_PORT_3 LODESTAR_HEAP SSV_DKG_PORT SIREN_PASSWORD )
  TARGET_VARS=( ETH_DOCKER_TAG NIM_SRC_BUILD_TARGET NIM_SRC_REPO NIM_DOCKER_TAG NIM_DOCKER_VC_TAG NIM_DOCKER_REPO \
    NIM_DOCKER_VC_REPO NIM_DOCKERFILE TEKU_SRC_BUILD_TARGET TEKU_SRC_REPO TEKU_DOCKER_TAG TEKU_DOCKER_REPO \
    TEKU_DOCKERFILE LH_SRC_BUILD_TARGET LH_SRC_REPO LH_DOCKER_TAG LH_DOCKER_REPO LH_DOCKERFILE \
    PRYSM_SRC_BUILD_TARGET PRYSM_SRC_REPO PRYSM_DOCKER_TAG PRYSM_DOCKER_VC_TAG PRYSM_DOCKER_CTL_TAG \
    PRYSM_DOCKER_REPO PRYSM_DOCKER_VC_REPO PRYSM_DOCKER_CTL_REPO PRYSM_DOCKERFILE ERIGON_SRC_BUILD_TARGET \
    ERIGON_SRC_REPO ERIGON_DOCKER_TAG ERIGON_DOCKER_REPO ERIGON_DOCKERFILE MEV_SRC_BUILD_TARGET MEV_SRC_REPO \
    MEV_DOCKERFILE MEV_DOCKER_TAG MEV_DOCKER_REPO NIMEL_SRC_BUILD_TARGET NIMEL_SRC_REPO NIMEL_DOCKER_TAG \
    NIMEL_DOCKER_REPO NIMEL_DOCKERFILE LS_SRC_BUILD_TARGET LS_SRC_REPO LS_DOCKER_TAG LS_DOCKER_REPO LS_DOCKERFILE \
    GETH_SRC_BUILD_TARGET GETH_SRC_REPO GETH_DOCKER_TAG GETH_DOCKER_REPO TRAEFIK_TAG DDNS_TAG \
    GETH_DOCKERFILE NM_SRC_BUILD_TARGET NM_SRC_REPO NM_DOCKER_TAG NM_DOCKER_REPO NM_DOCKERFILE \
    BESU_SRC_BUILD_TARGET BESU_SRC_REPO BESU_DOCKER_TAG BESU_DOCKER_REPO BESU_DOCKERFILE SSV_NODE_TAG CHARON_VERSION \
    DEPCLI_SRC_BUILD_TARGET DEPCLI_SRC_REPO DEPCLI_DOCKER_TAG W3S_DOCKER_TAG W3S_DOCKER_REPO \
    PG_DOCKER_TAG RETH_SRC_BUILD_TARGET RETH_SRC_REPO RETH_DOCKER_TAG RETH_DOCKER_REPO RETH_DOCKERFILE \
    GRANDINE_SRC_BUILD_TARGET GRANDINE_SRC_REPO GRANDINE_DOCKER_TAG GRANDINE_DOCKER_REPO GRANDINE_DOCKERFILE \
    SIREN_DOCKER_TAG SIREN_DOCKER_REPO SSV_DKG_TAG NODE_EXPORTER_IGNORE_MOUNT_REGEX )

  OLD_VARS=( )
  NEW_VARS=( )

# Always make sure we have a SIREN password
  var="SIREN_PASSWORD"
  SIREN_PASSWORD=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [ -z "${SIREN_PASSWORD}" ]; then
    SIREN_PASSWORD=$(head -c 8 /dev/urandom | od -A n -t u8 | tr -d '[:space:]' | sha256sum | head -c 32)
    set_value_in_env
  fi

  var=ENV_VERSION
  __target_ver=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "default.env" || true)
  __source_ver=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Aggressive prune to work around Docker grabbing old clients. Here so it doesn't get called during config
  if [[ "${__source_ver}" -lt "9" ]]; then
    dodocker system prune --force -a
  fi

  if [[ "${__keep_targets}" -eq 1 && "${__target_ver}" -le "${__source_ver}" ]]; then # No changes in template, do nothing
    return 0
  fi

  if [ "${__keep_targets}" -eq 0 ]; then
    echo "Refreshing build targets in ${ENV_FILE}"
  else
    echo "Migrating ${ENV_FILE} to version ${__target_ver}"
  fi
  ${__as_owner} cp "${ENV_FILE}" "${ENV_FILE}".source
  __during_migrate=1
  __migrated=1
  ${__as_owner} cp default.env "${ENV_FILE}"

  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
# Literal match intended
# shellcheck disable=SC2076
  if [[ "${value}" =~ "blox-ssv2.yml" ]]; then
    ssv_switch
  fi

  # Migrate over user settings
  for var in "${ALL_VARS[@]}"; do
    value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
    if [ -n "${value}" ] || [ "${var}" = "GRAFFITI" ] || [ "${var}" = "MEV_RELAYS" ] \
        || [ "${var}" = "ETH_DOCKER_TAG" ] || [ "${var}" = "RAPID_SYNC_URL" ]; then
      if [ "${var}" = "COMPOSE_FILE" ]; then
        migrate_compose_file
      fi
      if [ "${var}" = "CL_QUIC_PORT" ]; then
        __cl_port=$(sed -n -e "s/^CL_P2P_PORT=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
        if [ -n "${__cl_port}" ] && [ "${__cl_port}" = "${value}" ]; then
          value=$((value + 1))
          echo "Adjusted CL_QUIC_PORT to ${value} so it does not conflict with CL_P2P_PORT"
        fi
        __prysm_port=$(sed -n -e "s/^PRYSM_UDP_PORT=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
        if [ -n "${__prysm_port}" ] && [ "${__prysm_port}" = "${value}" ]; then # just in case this is one ahead
          value=$((value + 1))
          echo "Adjusted CL_QUIC_PORT to ${value} so it does not conflict with PRYSM_UDP_PORT"
        fi
      fi
# Literal match intended
# shellcheck disable=SC2076
      if [[ "${var}" = "RAPID_SYNC_URL" && "${value}" =~ "eth2-beacon-mainnet.infura.io" ]]; then
        value="https://beaconstate.info"
      fi
      if [[ "${var}" = "HOST_IP" && "${value: -1}" = ":" ]]; then
        value="${value%:}" # Undo Compose V1 accommodation
      fi
      if [[ "${var}" = "SHARE_IP" && "${value: -1}" = ":" ]]; then
        value="${value%:}" # Undo Compose V1 accommodation
      fi
      # Handle & in GRAFFITI gracefully
      sed -i'.original' -e "s~^\(${var}\s*=\s*\).*\$~\1${value//&/\\&}~" "${ENV_FILE}"
    else # empty value
      if [ "${var}" = "CF_ZONE_ID" ]; then
        __lookup_cf_zone
        if [ -n "${value}" ]; then
          sed -i'.original' -e "s~^\(${var}\s*=\s*\).*\$~\1${value//&/\\&}~" "${ENV_FILE}"
        fi
      fi
    fi
  done
  if [ "${__keep_targets}" -eq 1 ]; then
    # Migrate over build targets
    for var in "${TARGET_VARS[@]}"; do
      value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
      if [ -n "${value}" ]; then
        if [[ "${var}" = "DDNS_TAG" && "${__source_ver}" -lt "8" ]]; then # Switch to ddns-updater
          value="v2"
        fi
        if [[ "${var}" = "LH_DOCKER_TAG" && "${value}" = "latest-modern" ]]; then # LH 5.2 ditched latest-modern
          value="latest"
        fi
        if [[ "${var}" = "ERIGON_DOCKER_TAG" && "${value}" = "stable" ]]; then # Erigon ditched stable
          value="v2.60.1"
        fi
        sed -i'.original' -e "s~^\(${var}\s*=\s*\).*$~\1${value}~" "${ENV_FILE}"
      fi
    done
  fi
  # Move value from old variable name(s) to new one(s)
  for index in "${!OLD_VARS[@]}"; do
    var=${OLD_VARS[index]}
    value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}.source" || true)
    if [ -n "${value}" ]; then
      sed -i'.original' -e "s~^\(${NEW_VARS[index]}\s*=\s*\).*$~\1${value}~" "${ENV_FILE}"
    fi
  done
  # Check whether we run a CL or VC, if so nag about FEE_RECIPIENT
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  # It's CL&VC, CL-only, or VC-only
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ "${value}" =~ "prysm.yml" || "${value}" =~ "lighthouse.yml" || "${value}" =~ "teku.yml" \
    || "${value}" =~ "nimbus.yml" || "${value}" =~ "lodestar.yml" || "${value}" =~ "-cl-only.yml" \
    || "${value}" =~ "-allin1.yml" || "${value}" =~ "-vc-only.yml" ]]; then
    # Check for rewards
    var="FEE_RECIPIENT"
    value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    if [[ -z "${value}" || ${value} != 0x* || ${#value} -ne 42 ]]; then
      if [ "${__non_interactive:-0}" -eq 0 ]; then
        whiptail --msgbox "A fee recipient ETH wallet address is required in order to start the client. This is \
  for priority fees and, optionally, MEV. Please enter a valid ETH address in the next screen. Refer to \
  Eth Docker docs (https://ethdocker.com/About/Rewards) for more information.\n\nCAUTION: \"$__me up\" will fail if no \
  valid address is set" 12 75
        query_coinbase
        set_value_in_env
      else
        echo "A fee recipient ETH wallet address is required in order to start the client. Please set one in \".env\"."
        echo "CAUTION: \"$__me up\" will fail if no valid address is set."
      fi
    fi
  fi

  # User signals it's a distributed setup and not to nag
  var="DISTRIBUTED"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [[ "${value}" = "true" || "${__non_interactive:-0}" -eq 1 ]]; then
    ${__as_owner} rm "${ENV_FILE}".original
    __during_migrate=0
    return 0
  fi
  # Check for CL and EL, nag if we have only one without the other
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  # Case 1 ... CL, do we have an EL?
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ "${value}" =~ "prysm.yml" || "${value}" =~ "lighthouse.yml" || "${value}" =~ "teku.yml" \
    || "${value}" =~ "nimbus.yml" || "${value}" =~ "lodestar.yml" || "${value}" =~ "-cl-only.yml" \
    || "${value}" =~ "-allin1.yml" ]]; then
    if [[ ! "${value}" =~ "geth.yml" && ! "${value}" =~ "besu.yml" && ! "${value}" =~ "erigon.yml" \
      && ! "${value}" =~ "nethermind.yml"  && ! "${value}" =~ "nimbus-el.yml" \
      && ! "${value}" =~ "reth.yml" ]]; then
      whiptail --msgbox "An Execution Layer client is required alongside your Consensus Layer client since \
Ethereum Merge.\n\nIf you run a distributed setup, you can shut off this nag screen by setting DISTRIBUTED=true in \
${ENV_FILE}" 12 75
    fi
# Case 2 ... EL, do we have a CL?
  elif [[ "${value}" =~ "geth.yml" || "${value}" =~ "besu.yml" || "${value}" =~ "erigon.yml" \
    || "${value}" =~ "nethermind.yml" || "${value}" =~ "nimbus-el.yml" || "${value}" =~ "reth.yml" ]]; then
    if [[ ! "${value}" =~ "prysm.yml"  && ! "${value}" =~ "lighthouse.yml"  && ! "${value}" =~ "teku.yml"  \
      && ! "${value}" =~ "nimbus.yml"  && ! "${value}" =~ "lodestar.yml"  && ! "${value}" =~ "-cl-only.yml" \
      && ! "${value}" =~ "-allin1.yml" ]]; then
      whiptail --msgbox "A Consensus Layer client is required alongside your Execution Layer client since \
Ethereum Merge.\n\nIf you run a distributed setup, you can shut off this nag screen by setting DISTRIBUTED=true in \
${ENV_FILE}" 12 75
    fi
  fi

  ${__as_owner} rm "${ENV_FILE}".original
  __during_migrate=0
  echo "${ENV_FILE} updated successfully"
}


nag_os_version() {
  if [[ "$__distro" = "ubuntu" ]]; then
    if [ "${__os_major_version}" -lt 22 ]; then
     echo
     echo "Ubuntu ${__os_major_version} is older than the recommended 24.04 or 22.04 version."
     echo
    fi
  fi

  if [[ "$__distro" =~ "debian" ]]; then
    if [ "${__os_major_version}" -lt 11 ]; then
     echo
     echo "Debian ${__os_major_version} is older than the recommended 12 or 11 version."
     echo
    fi
  fi
}


pull_and_build() {
  dodocker system prune --force
  docompose --profile tools pull
  source_build
  docompose --profile tools build --pull
}


# Arguments are passed, but shellcheck doesn't recognize that
# shellcheck disable=SC2120
update() {
  __during_update=1

  if [[ $(${__as_owner} git status --porcelain) ]]; then
    __dirty=1
  else
    __dirty=0
  fi

  __free_space=$(df -P "$(pwd)" | awk '/[0-9]%/{print $(NF-2)}')

  re='^[0-9]+$'
  if ! [[ "${__free_space}" =~ $re ]] ; then
    echo "Unable to determine free disk space. This is likely a bug."
    echo "df reports $(df -P "$(pwd)") and __free_space is ${__free_space}"
    exit 70
  elif [ "$(df -P "$(pwd)" | awk '/[0-9]%/{print $(NF-2)}')" -lt 1024 ]; then
    echo "You have less than 1 MiB of space left on $(pwd)."
    echo "Aborting, as an update is not safe."
    exit 1
  fi

  __get_docker_free_space
  if [ "${__free_space}" -lt 1048576 ]; then
    echo "You have less than 1 GiB of space left for Docker volumes."
    echo "Aborting, as an update is not safe."
    exit 1
  fi

  if [ -z "${ETHDSECUNDO-}" ]; then
    set +e
    ${__as_owner} git config pull.rebase false
    var="ETH_DOCKER_TAG"
    value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    if [ -z "${value}" ] || [ "${value}" = "latest" ]; then
      export ETHDPINNED=""
      __branch=$(git rev-parse --abbrev-ref HEAD)
      if [[ "${__branch}" =~ ^tag-v[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        git checkout main
      fi
# This preps for a removal of ext-network.yml in a future update, after Pectra
      ${__as_owner} cp ext-network.yml ext-network.yml.bak
      if git ls-files -v | grep -q "^h" | grep -q "ext-network.yml$"; then
        ${__as_owner} git update-index --no-assume-unchanged ext-network.yml
      fi
      ${__as_owner} git restore ext-network.yml
      ${__as_owner} git pull origin main
      ${__as_owner} cp ext-network.yml.bak ext-network.yml
      if git ls-files -v | grep -q "ext-network.yml$"; then
        ${__as_owner} git update-index --assume-unchanged ext-network.yml
      fi
    else
      export ETHDPINNED="${value}"
      ${__as_owner} git fetch --tags
      ${__as_owner} git checkout -B "tag-${value}" "tags/${value}"
    fi
    export GITEXITCODE=$?
    set -e
    # BASH_SOURCE so newer code gets to do the update. Use an ENV var
    # to avoid infinite loop
    export ETHDSECUNDO=1
    exec "${BASH_SOURCE[0]}" update "$@"
  fi

  __keep_targets=1
  __targetcli=""
  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --keep-targets)
        if [ -n "${__targetcli}" ]; then
          echo "Error: --keep-targets and --refresh-targets cannot be used together; use either option once only"
          exit 1
        fi
        __keep_targets=1
        __targetcli="--keep-targets"
        shift
        ;;
      --refresh-targets | --reset-targets)
        if [ -n "${__targetcli}" ]; then
          echo "Error: --keep-targets and --refresh-targets cannot be used together; use either option once only"
          exit 1
        fi
        __keep_targets=0
        __targetcli="--refresh-targets"
        shift
        ;;
      --non-interactive)
        __non_interactive=1
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        shift
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi

# envmigrate used to be called w/ arguments and checks for that
# shellcheck disable=SC2119
  envmigrate
  pull_and_build

  delete_erigon
  delete_reth
  upgrade_postgres

  echo
  if [ "${__migrated}" -eq 1 ] && ! cmp -s "${ENV_FILE}" "${ENV_FILE}".source; then
    ${__as_owner} cp "${ENV_FILE}".source "${ENV_FILE}".bak
    ${__as_owner} rm "${ENV_FILE}".source
    echo "Your ${ENV_FILE} configuration settings have been migrated to a fresh copy. You can \
find the original contents in ${ENV_FILE}.bak."
    if [ "${__keep_targets}" -eq 0 ]; then
      echo "NB: If you made changes to the source or binary build targets, these have been \
reset to defaults."
    fi
    echo
    echo "List of changes made to ${ENV_FILE} during migration - current on left, original on right:"
    echo
    diff -y --suppress-common-lines "${ENV_FILE}" "${ENV_FILE}".bak || true
  else
    echo "No changes made to ${ENV_FILE} during update"
    if [ -f "${ENV_FILE}".source ]; then
      ${__as_owner} rm "${ENV_FILE}".source || true
    fi
  fi
  echo
  if [ -z "${GITEXITCODE+x}" ] || [ "${GITEXITCODE}" -eq 0 ]; then
    echo "An \"$__me up\" command will start using the new images and configuration."
  else
    echo "WARNING"
    echo
    echo "Updating ${__project_name} failed during \"git pull\""
    echo
    echo "Please try \"git pull\" manually."
    echo "Do not run \"$__me up\" until git can update ${__project_name}."
    echo "The current partial update risks startup failure."
  fi

  nag_os_version

  unset ETHDSECUNDO
  unset GITEXITCODE
  if [ "${__dirty}" -eq 1 ]; then
    echo
    echo "WARNING"
    echo
    echo "You have uncommitted local changes to ${__project_name}, which may interfere with updates."
    echo "Please undo these changes or \"git commit\" them."
    echo "These are the files with local changes:"
    echo
    ${__as_owner} git status --porcelain
  fi
  if [ -n "${ETHDPINNED:-}" ]; then
    echo "${__project_name} version is pinned to ${ETHDPINNED} in \".env\"."
    echo "Please make sure to run compatible client versions."
  fi
  __during_update=0
}


resync-execution() {
# Check for EL client
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  case "${value}" in
    *erigon.yml* ) __el_volume='erigon-el-data'; __el_client="erigon";;
    *geth.yml* ) __el_volume='geth-el-data'; __el_client="geth";;
    *reth.yml* ) __el_volume='reth-el-data'; __el_client="reth";;
    *besu.yml* ) __el_volume='besu-el-data'; __el_client="besu";;
    *nethermind.yml* ) __el_volume='nethermind-el-data'; __el_client="nethermind";;
    * ) echo "You do not appear to be running an execution layer client. Nothing to do."; return 0;;
  esac

  if ! dodocker volume ls -q | grep -q "$(basename "$(realpath .)")[_-]${__el_volume}"; then
    echo "Did not find Docker volume for ${__el_client}. Nothing to do."
    return 0
  fi

  echo "This will stop ${__el_client} and delete its database to force a resync."
  read -rp "WARNING - resync may take days. Do you wish to continue? (No/yes) " yn
  case $yn in
    [Yy][Ee][Ss] ) ;;
    * ) echo "Aborting."; exit 130;;
  esac

  __el_volume="$(basename "$(realpath .)")_${__el_volume}"
  echo "Stopping ${__el_client} container"
  docompose stop execution && docompose rm -f execution
  dodocker volume rm "$(dodocker volume ls -q -f "name=${__el_volume}")"
  __volume_id=""
  if [[ "${__el_volume}" =~ geth-el-data ]]; then
    __legacy_volume="$(basename "$(realpath .)")_geth-eth1-data"
    __volume_id="$(dodocker volume ls -q -f "name=${__legacy_volume}")"
  elif [[ "${__el_volume}" =~ besu-el-data ]]; then
    __legacy_volume="$(basename "$(realpath .)")_besu-eth1-data"
    __volume_id="$(dodocker volume ls -q -f "name=${__legacy_volume}")"
  elif [[ "${__el_volume}" =~ nethermind-el-data ]]; then
    __legacy_volume="$(basename "$(realpath .)")_nm-eth1-data"
    __volume_id="$(dodocker volume ls -q -f "name=${__legacy_volume}")"
  fi
  if [ -n "${__volume_id}" ]; then
    dodocker volume rm "${__volume_id}"
  fi
  echo
  echo "${__el_client} stopped and database deleted."
  echo
  echo "Restarting for resync."
  up
}


resync-consensus() {
  # Check for CL client
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  case "${value}" in
    *lighthouse.yml* | *lighthouse-cl-only.yml* ) __cl_volume='lhconsensus-data'; __cl_client="lighthouse";;
    *teku-allin1.yml* ) __cl_volume='wipe-db'; __cl_client="teku";;
    *teku.yml* | *teku-cl-only.yml* ) __cl_volume='tekuconsensus-data'; __cl_client="teku";;
    *nimbus-allin1.yml* ) __cl_volume='wipe-db'; __cl_client="nimbus";;
    *nimbus.yml* | *nimbus-cl-only.yml* ) __cl_volume='nimbus-consensus-data'; __cl_client="nimbus";;
    *lodestar.yml* | *lodestar-cl-only.yml* ) __cl_volume='lsconsensus-data'; __cl_client="lodestar";;
    *prysm.yml* | *prysm-cl-only.yml* ) __cl_volume='prysmconsensus-data'; __cl_client="prysm";;
    *grandine-allin1.yml* ) __cl_volume='wipe-db'; __cl_client="grandine";;
    *grandine.yml* | *grandine-cl-only.yml* ) __cl_volume='grandineconsensus-data'; __cl_client="grandine";;
    * ) echo "You do not appear to be running a consensus layer client. Nothing to do."; return;;
  esac

  if [ ! "${__cl_volume}" = "wipe-db" ] && ! dodocker volume ls -q \
      | grep -q "$(basename "$(realpath .)")[_-]${__cl_volume}"; then
    echo "Did not find Docker volume for ${__cl_client}. Nothing to do."
    return 0
  fi

  # Can we checkpoint sync?
  var="RAPID_SYNC_URL"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  echo "This will stop ${__cl_client} and delete its database to force a resync."
  if [ -z "${value}" ]; then
    read -rp "WARNING - RAPID_SYNC_URL not set, resync may take days. Do you wish to continue? (No/yes) " yn
  else
    read -rp "RAPID_SYNC_URL set, resync should finish in minutes. Do you wish to continue? (No/yes) " yn
  fi
  case $yn in
    [Yy][Ee][Ss] ) ;;
    * ) echo "Aborting."; exit 130;;
  esac

  echo "Stopping ${__cl_client} container"
  docompose stop consensus && docompose rm -f consensus
  if [ "${__cl_volume}" = "wipe-db" ]; then
    docompose run --rm wipe-db
  else
    __cl_volume="$(basename "$(realpath .)")_${__cl_volume}"
    dodocker volume rm "$(dodocker volume ls -q -f "name=${__cl_volume}")"
    __volume_id=""
    if [[ "${__cl_volume}" =~ lhconsensus-data ]]; then
      __legacy_volume="$(basename "$(realpath .)")_lhbeacon-data"
      __volume_id="$(dodocker volume ls -q -f "name=${__legacy_volume}")"
    elif [[ "${__cl_volume}" =~ prysmconsensus-data ]]; then
      __legacy_volume="$(basename "$(realpath .)")_prysmbeacon-data"
      __volume_id="$(dodocker volume ls -q -f "name=${__legacy_volume}")"
    fi
    if [ -n "${__volume_id}" ]; then
      dodocker volume rm "${__volume_id}"
    fi
  fi
  echo
  echo "${__cl_client} stopped and database deleted."
  echo
  echo "Restarting for resync."
  up
}


attach-geth() {
  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  if ! grep -q '^COMPOSE_FILE=.*geth\.yml' "${ENV_FILE}" 2>/dev/null ; then
    echo "You do not appear to be using Geth, aborting."
    exit 1
  fi
  __legacy_datadir=$(dodocker run --rm -v "$(dodocker volume ls -q -f \
    "name=$(basename "$(realpath .)")[_-]geth-eth1-data")":"/var/lib/goethereum" \
    alpine:3 sh -c 'if [ -d "/var/lib/goethereum/geth/chaindata" ]; then echo true; else echo false; fi')

  if [ "${__legacy_datadir}" = "true" ]; then
    docompose exec -it execution bash -c "geth attach /var/lib/goethereum/geth.ipc"
  else
    docompose exec -it execution bash -c "geth attach /var/lib/geth/geth.ipc"
  fi
}


prune-besu() {
  __non_interactive=0
  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --non-interactive)
        __non_interactive=1
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        exit 1
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi

  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  if ! grep -q '^COMPOSE_FILE=.*besu\.yml' "${ENV_FILE}" 2>/dev/null ; then
    echo "You do not appear to be using Besu, aborting."
    exit 1
  fi

  # Check for archive node
  var="ARCHIVE_NODE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [[ "${value}" = "true" ]]; then
    echo "Besu is an archive node: Aborting."
    exit 1
  fi

  rpc_line=$(grep '^EL_RPC_PORT=' "${ENV_FILE}")
  regex='^EL_RPC_PORT=([0-9]+)'
  if [[ ! "${rpc_line}" =~ ${regex} ]]; then
    echo "Unable to determine EL_RPC_PORT, aborting."
    exit 1
  else
    rpc_port="${BASH_REMATCH[1]}"
  fi

  set +e
  sync_status=$(docompose exec -T execution wget -qO- "http://localhost:$rpc_port" \
  --header 'Content-Type: application/json' --post-data '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}')
  exitstatus=$?
  set -e
  if [ $exitstatus -ne 0 ]; then
    echo "Unable to connect to Besu: Is it running?"
    echo "Output: ${sync_status}"
    echo "Aborting."
    exit 1
  fi

  if [[ ! "${sync_status}" =~ "false" ]]; then
    echo "Besu is not done syncing yet. Sync status:"
    echo "${sync_status}"
    echo
    echo "Aborting."
    exit 1
  fi

  if [ $__non_interactive = 0 ]; then
    while true; do
      read -rp "WARNING - this will stop Besu and prune its trie-logs. Do you wish to continue? (No/Yes) " yn
      case $yn in
        [Yy][Ee][Ss] ) break;;
        * ) echo "Aborting, no changes made"; exit 130;;
      esac
    done
  fi

  echo
  echo "Starting Besu prune"
  echo
  docompose run --rm set-prune-marker "touch /var/lib/besu/prune-marker"
  docompose stop execution && docompose rm -f execution
  start
  echo
  echo "Prune is running, you can observe it with '$__me logs -f execution'"
  echo
  echo "When prune is done, Besu will automatically start again."
  echo
}


prune-reth() {
  __non_interactive=0
  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --non-interactive)
        __non_interactive=1
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        exit 1
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi

  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  if ! grep -q '^COMPOSE_FILE=.*reth\.yml' "${ENV_FILE}" 2>/dev/null ; then
    echo "You do not appear to be using Reth, aborting."
    exit 1
  fi

  # Check for archive node
  var="ARCHIVE_NODE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [[ "${value}" = "true" ]]; then
    echo "Reth is an archive node: Aborting."
    exit 1
  fi

  rpc_line=$(grep '^EL_RPC_PORT=' "${ENV_FILE}")
  regex='^EL_RPC_PORT=([0-9]+)'
  if [[ ! "${rpc_line}" =~ ${regex} ]]; then
    echo "Unable to determine EL_RPC_PORT, aborting."
    exit 1
  else
    rpc_port="${BASH_REMATCH[1]}"
  fi

  set +e
  sync_status=$(docompose exec -T execution wget -qO- "http://localhost:$rpc_port" \
  --header 'Content-Type: application/json' --post-data '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}')
  exitstatus=$?
  set -e
  if [ $exitstatus -ne 0 ]; then
    echo "Unable to connect to Reth: Is it running?"
    echo "Output: ${sync_status}"
    echo "Aborting."
    exit 1
  fi

  if [[ ! "${sync_status}" =~ "false" ]]; then
    echo "Reth is not done syncing yet. Sync status:"
    echo "${sync_status}"
    echo
    echo "Aborting."
    exit 1
  fi

  if [ $__non_interactive = 0 ]; then
    while true; do
      read -rp "WARNING - this will stop Reth and prune its database. Do you wish to continue? (No/Yes) " yn
      case $yn in
        [Yy][Ee][Ss] ) break;;
        * ) echo "Aborting, no changes made"; exit 130;;
      esac
    done
  fi

  echo
  echo "Starting Reth prune"
  echo
  docompose run --rm set-prune-marker "touch /var/lib/reth/prune-marker"
  docompose stop execution && docompose rm -f execution
  start
  echo
  echo "Prune is running, you can observe it with '$__me logs -f execution'"
  echo
  echo "When prune is done, Reth will automatically start again."
  echo
}


prune-nethermind() {
  __non_interactive=0
  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --non-interactive)
        __non_interactive=1
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        exit 1
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi

  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  if ! grep -q '^COMPOSE_FILE=.*nethermind\.yml' "${ENV_FILE}" 2>/dev/null ; then
    echo "You do not appear to be using Nethermind, aborting."
    exit 1
  fi

  # Check for archive node
  var="ARCHIVE_NODE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [[ "${value}" = "true" ]]; then
    echo "Nethermind is an archive node: Aborting."
    exit 1
  fi

  __get_docker_free_space

  var="NETWORK"
  NETWORK=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  if [ "${NETWORK}" = "mainnet" ] || [ "${NETWORK}" = "gnosis" ]; then
    __min_free=262144000
    __min_gib=250
  else
    __min_free=26214400
    __min_gib=25
  fi

  if [ "${__free_space}" -lt ${__min_free} ]; then
    echo "You do not have enough free disk space. Make sure this reads at least ${__min_gib}G free (Avail):"
    echo
    __display_docker_dir
    echo
    echo "Aborting."
    exit 1
  fi

  rpc_line=$(grep '^EL_RPC_PORT=' "${ENV_FILE}")
  regex='^EL_RPC_PORT=([0-9]+)'
  if [[ ! "${rpc_line}" =~ ${regex} ]]; then
    echo "Unable to determine EL_RPC_PORT, aborting."
    exit 1
  else
    rpc_port="${BASH_REMATCH[1]}"
  fi

  set +e
  sync_status=$(docompose exec -T execution wget -qO- "http://localhost:$rpc_port" --header \
    'Content-Type: application/json' --post-data '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}')
  exitstatus=$?
  set -e
  if [ $exitstatus -ne 0 ]; then
    echo "Unable to connect to Nethermind: Is it running?"
    echo "Output: ${sync_status}"
    echo "Aborting."
    exit 1
  fi

  if [[ ! "${sync_status}" =~ "false" ]]; then
    echo "Nethermind is not done syncing yet. Sync status:"
    echo "${sync_status}"
    echo
    echo "Aborting."
    exit 1
  fi

  var="AUTOPRUNE_NM"
  auto_prune=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  if [ $__non_interactive = 0 ]; then
    while true; do
      if [ "${auto_prune}" = true ]; then
        if [ "${NETWORK}" = "mainnet" ] || [ "${NETWORK}" = "gnosis" ]; then
          threshold="350"
        else
          threshold="50"
        fi
        echo "Nethermind should auto-prune below ${threshold} GiB free. Check logs with \"$__me logs -f --tail 500 \
execution | grep Full\" to see whether it is."
      fi
      read -rp "WARNING - this will prune Nethermind's database in the background. Do you wish to continue? (No/Yes) " yn
      case $yn in
        [Yy][Ee][Ss] ) break;;
        * ) echo "Aborting, no changes made"; exit 130;;
      esac
    done
  fi

  echo
  echo "Starting Nethermind prune"
  echo

  set +e
  prune_result=$(docompose exec -T execution wget -qO- "http://localhost:1337" --header \
  'Content-Type: application/json' --post-data '{"jsonrpc":"2.0","method":"admin_prune","params":[],"id":1}')
  exitstatus=$?
  set -e
  if [ $exitstatus -ne 0 ]; then
    echo "Unable to start prune, error code ${exitstatus}. This is likely a bug."
    echo "An attempt to run it returned this: ${prune_result}"
    # shellcheck disable=SC2028
    echo 'The command attempted was: docker compose run --rm set-prune-marker "curl -s \
--data {\\\"method\\\":\\\"admin_prune\\\",\\\"params\\\":[],\\\"id\\\":1,\\\"jsonrpc\\\":\\\"2.0\\\"} \
-H Content-Type:\ application/json http://execution:8545"'
    exit ${exitstatus}
  fi
  echo "Nethermind returns ${prune_result}"
  if [[ ! "${prune_result}" =~ [Ss]tarting ]]; then
    echo "Unable to start prune. This is likely a bug."
    exit 70
  fi
  echo
  echo "Prune is running, you can observe it with \"$__me logs -f --tail 500 execution | grep Full\""
  echo
  echo "Please do not restart the node or restart Nethermind until prune is done."
  echo
  echo "When prune is done, Nethermind will automatically re-start."
  echo
}


prune-lighthouse() {
  __non_interactive=0
  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --non-interactive)
        __non_interactive=1
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        exit 1
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi

  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Literal match intended
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "lighthouse.yml" && ! "${value}" =~ "lighthouse-cl-only.yml" ]]; then
    echo "You do not appear to be using Lighthouse, aborting."
    exit 1
  fi

  # Check for archive node
  var="ARCHIVE_NODE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [[ "${value}" = "true" ]]; then
    echo "Lighthouse is an archive node: Aborting."
    exit 1
  fi

  rpc_line=$(grep '^CL_REST_PORT=' "${ENV_FILE}")
  regex='^CL_REST_PORT=([0-9]+)'
  if [[ ! "${rpc_line}" =~ ${regex} ]]; then
    echo "Unable to determine CL_REST_PORT, aborting."
    exit 1
  else
    rpc_port="${BASH_REMATCH[1]}"
  fi

  set +e
  sync_status=$(docompose exec -T consensus wget -qO- "http://localhost:$rpc_port/eth/v1/node/syncing")
  exitstatus=$?
  set -e
  if [ $exitstatus -ne 0 ]; then
    echo "Unable to connect to Lighthouse: Is it running?"
    echo "Output: ${sync_status}"
    echo "Aborting."
    exit 1
  fi

  if [[ "${sync_status}" =~ "true" ]]; then # Avoid jq - if el_offline or is_optimistic or is_syncing, don't proceed
    echo "Lighthouse is not done syncing yet. Sync status:"
    echo "${sync_status}"
    echo
    echo "Aborting."
    exit 1
  fi

  if [ $__non_interactive = 0 ]; then
    while true; do
      read -rp "WARNING - this will stop Lighthouse and prune its state. Do you wish to continue? (No/Yes) " yn
      case $yn in
        [Yy][Ee][Ss] ) break;;
        * ) echo "Aborting, no changes made"; exit 130;;
      esac
    done
  fi

  echo
  echo "Starting Lighthouse prune"
  echo
  docompose run --rm set-cl-prune-marker "touch /var/lib/lighthouse/beacon/prune-marker"
  docompose stop consensus && docompose rm -f consensus
  start
  echo
  echo "Prune is running, you can observe it with '$__me logs -f consensus'"
  echo
  echo "When prune is done, Lighthouse will automatically start again."
  echo
}


prep-keyimport() {
  if [ ! -f "${ENV_FILE}" ]; then
    echo "${ENV_FILE} configuration file not found, aborting."
    exit 1
  fi

  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Literal match intended
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "prysm.yml" ]] && [[ ! "${value}" =~ "lighthouse.yml" ]] && [[ ! "${value}" =~ "teku.yml" ]] \
       && [[ ! "${value}" =~ "nimbus.yml" ]] && [[ ! "${value}" =~ "lodestar.yml" ]] && \
       [[ ! "${value}" =~ "-allin1.yml" ]] && [[ ! "${value}" =~ "vc-only.yml" ]]; then
    echo "You do not appear to be running a validator client. Aborting."
    exit 1
  fi

  __args=""

  while :
  do
    if [ -z "${1+x}" ]; then
      break
    fi
    case "$1" in
      --path)
        if [ -z "${2+x}" ]; then
          echo "--path requires a directory path, aborting"
          exit 1
        fi
        if [ ! -d "$2" ]; then
          echo "$2 is not a directory"
          exit 1
        fi
        if [ "$(realpath "$2")" = "$(realpath ".eth/validator_keys")" ]; then
          echo "$2 is the default path, doing nothing special"
          shift 2
          continue
        fi
        IFS=$'\n'
        files=$(find "$2" -maxdepth 1 -name '*.json')
        # Unset restores default
        unset IFS
        if [ -z "$files" ]; then
          echo "No .json files found in $2, aborting"
          exit 1
        fi
        IFS=$'\n'
        files=$(find ./.eth/validator_keys -maxdepth 1 -name '*.json')
        # Unset restores default
        unset IFS
        if [ -n "$files" ]; then
          ${__as_owner} mkdir -p ./.eth/validator_keys/keybackup
          ${__as_owner} mv -uf ./.eth/validator_keys/*.json ./.eth/validator_keys/keybackup
          ${__as_owner} rm -f ./.eth/validator_keys/*.json
          echo "Moved existing json files to .eth/validator_keys/keybackup"
        fi
        ${__as_owner} cp "$2"/*.json .eth/validator_keys/
        shift 2
        ;;
      --non-interactive)
        if [ -z "${KEYSTORE_PASSWORD+x}" ]; then
          echo "KEYSTORE_PASSWORD not set or empty, aborting"
          exit 1
        fi
        __args+="${__args:+ }--interactive"
        shift
        ;;
      --debug)
        __args+="${__args:+ }--debug"
        shift
        ;;
      *)
        echo "Error: Unknown option: $1" >&2
        exit 1
        ;;
    esac
  done
  if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
    __non_interactive=1
  fi
}


__i_haz_ethdo() {
  if [ ! -f "${ENV_FILE}" ]; then
    echo "${__project_name} has not been configured. Please run $__me config first."
    exit 0
  fi
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Literal match intended
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "ethdo.yml" ]]; then
    echo "Please edit the ${ENV_FILE} file and make sure \":ethdo.yml\" is added to the \"COMPOSE_FILE\" line"
    echo "For example, \"nano ${ENV_FILE}\" will open the nano text editor with the \"${ENV_FILE}\" file loaded."
    echo "Without it, this step cannot be run"
    echo
    read -rp "Do you want me to make this change for you? (n/y)" yn
    case $yn in
      [Yy] );;
      * ) exit 130;;
    esac
    if [ -n "${value}" ]; then
      COMPOSE_FILE="${value}:ethdo.yml"
    else
      COMPOSE_FILE="ethdo.yml"
      echo "You do not have a CL in ${__project_name}. Please make sure CL_NODE in ${ENV_FILE} points at an available one"
    fi
    set_value_in_env
    echo "Your COMPOSE_FILE now reads ${COMPOSE_FILE}"
  fi
}


__i_haz_web3signer() {
  if [ ! -f "${ENV_FILE}" ]; then
    echo "${__project_name} has not been configured. Please run $__me config first."
    exit 0
  fi

  var="WEB3SIGNER"
  __w3s=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [ ! "${__w3s}" = "true" ]; then
    return 0
  fi

  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Literal match intended
# shellcheck disable=SC2076
  if [[ ! "${value}" =~ "web3signer.yml" ]]; then
    echo "WEB3SIGNER=true in ${ENV_FILE}, but web3signer.yml is not in use"
    echo "Please edit the ${ENV_FILE} file and make sure \":web3signer.yml\" is added to the \"COMPOSE_FILE\" line"
    echo "For example, \"nano ${ENV_FILE}\" will open the nano text editor with the \"${ENV_FILE}\" file loaded."
    echo "Without it, $__me keys cannot be run"
    echo
    read -rp "Do you want me to make this change for you? (n/y)" yn
    case $yn in
      [Yy] );;
      * ) exit 130;;
    esac
    if [ -n "${value}" ]; then
      COMPOSE_FILE="${value}:web3signer.yml"
    else
      echo "You do not have a validator client in ${__project_name}. web3signer cannot be used without one."
      exit 1
    fi
    set_value_in_env
    echo "Your COMPOSE_FILE now reads ${COMPOSE_FILE}"
  fi
}


__i_haz_keys_service() {
# This caused issues and is currently not being called
  if ! docompose --profile tools config --services | grep -q validator-keys; then
    if [[ "${1:-}" = "silent" ]]; then
      return 1
    fi
    echo "The validator-keys service is not defined. Are you running only a consensus layer client?"
    echo "Key management happens on the validator client / web3signer, not the consensus layer client."
    echo "You can however do things like send an exit message, prep a withdrawal credentials change,"
    echo "sign exit messages with ethdo."
    echo
    echo "Aborting."
    exit 1
  fi
  return 0
}


__keys_usage() {
  echo "Call keymanager with an ACTION, one of:"
  echo "  create-for-csm"
  echo "      Create keys for Lido CSM"
  echo "  list"
  echo "      Lists the public keys of all validators currently loaded into your validator client"
  echo "  import"
  echo "      Import all keystore*.json in .eth/validator_keys while loading slashing protection data"
  echo "      in slashing_protection*.json files that match the public key(s) of the imported validator(s)"
  echo "  delete 0xPUBKEY | all"
  echo "      Deletes the validator with public key 0xPUBKEY from the validator client, and exports its"
  echo "      slashing protection database."
  echo "      \"all\" deletes all detected validators."
  echo "  register"
  echo "      For use with web3signer only: Re-register all keys in web3signer with the validator client"
  echo
  echo "  get-recipient 0xPUBKEY"
  echo "      List fee recipient set for the validator with public key 0xPUBKEY"
  echo "      Validators will use FEE_RECIPIENT in ${ENV_FILE} by default, if not set individually"
  echo "  set-recipient 0xPUBKEY 0xADDRESS"
  echo "      Set individual fee recipient for the validator with public key 0xPUBKEY"
  echo "  delete-recipient 0xPUBKEY"
  echo "      Delete individual fee recipient for the validator with public key 0xPUBKEY"
  echo
  echo "  get-gas 0xPUBKEY"
  echo "      List execution gas limit set for the validator with public key 0xPUBKEY"
  echo "      Validators will use the client's default, if not set individually"
  echo "  set-gas 0xPUBKEY amount"
  echo "      Set individual execution gas limit for the validator with public key 0xPUBKEY"
  echo "  delete-gas 0xPUBKEY"
  echo "      Delete individual execution gas limit for the validator with public key 0xPUBKEY"
  echo
  echo "  get-graffiti 0xPUBKEY"
  echo "      List graffiti set for the validator with public key 0xPUBKEY"
  echo "      Validators will use GRAFFITI in .env by default, if not set individually"
  echo "  set-graffiti 0xPUBKEY amount"
  echo "      Set individual graffiti for the validator with public key 0xPUBKEY"
  echo "  delete-graffiti 0xPUBKEY"
  echo "      Delete individual graffiti for the validator with public key 0xPUBKEY"
  echo
  echo "  get-api-token"
  echo "      Print the token for the keymanager API running on port ${KEY_API_PORT:-7500}."
  echo "      This is also the token for the Prysm Web UI"
  echo
  echo "  create-prysm-wallet"
  echo "      Create a new Prysm wallet to store keys in"
  echo "  get-prysm-wallet"
  echo "      Print Prysm's wallet password"
  echo
  echo "  get-grandine-wallet"
  echo "      Print Grandine's wallet password"
  echo
  echo "  prepare-address-change"
  echo "      Create an offline-preparation.json with ethdo"
  echo "  send-address-change"
  echo "      Send a change-operations.json with ethdo, setting the withdrawal address"
  echo
  echo "  sign-exit 0xPUBKEY | all"
  echo "      Create pre-signed exit message for the validator with public key 0xPUBKEY"
  echo "      \"all\" signs an exit message for all detected validators"
  echo "  sign-exit from-keystore [--offline]"
  echo "      Create pre-signed exit messages with ethdo, from keystore files in ./.eth/validator_keys"
  echo "  send-exit"
  echo "      Send pre-signed exit messages in ./.eth/exit_messages to the Ethereum chain"
  echo
  echo " Commands can be appended with \"--debug\" to see debug output"
}


keys() {
  if [[ "$#" -eq 0 || "$1" == "help" || "$1" == "-h" || "$1" == "--help" ]]; then
    __keys_usage
    return 0
  fi

  __i_haz_web3signer

  __owner_uid=$(id -u "${OWNER}")
  if [ "${1:-}" = "import" ]; then
    #__i_haz_keys_service
    shift
    prep-keyimport "$@"
    docompose run --rm -e OWNER_UID="${__owner_uid}" validator-keys import "${__args}"
  elif [ "${1:-}" = "create-prysm-wallet" ]; then
    var="COMPOSE_FILE"
    value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
# Literal match intended
# shellcheck disable=SC2076
    if [[ ! "${value}" =~ "prysm.yml" ]] && [[ ! "${value}" =~ "prysm-vc-only.yml" ]]; then
      echo "You do not appear to be using a Prysm validator. Aborting."
      exit 1
    fi
    if docompose run --rm create-wallet; then
      docompose stop validator
      docompose rm --force validator
      up
    fi
  elif [ "${1:-}" = "create-for-csm" ]; then
    var="NETWORK"
    NETWORK=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    query_lido_keys_generation
  elif [ "${1:-}" = "prepare-address-change" ]; then
    __i_haz_ethdo
    echo "Generating offline prep file"
    set +e
    docompose run --rm ethdo validator credentials set --prepare-offline
    exitstatus=$?
    set -e
    if [ "${exitstatus}" -ne 0 ]; then
      echo "Running ethdo failed, unfortunately. Is the CL running and synced?"
      echo "Please try again after fixing root cause. Aborting."
      exit 1
    fi
    echo
    echo "Downloading ethdo"
    REPO="wealdtech/ethdo"; \
    wget -q -O- https://api.github.com/repos/${REPO}/releases/latest | grep "browser_download_url.*linux-amd64.tar.gz" \
      | head -1 \
      | cut -d : -f 2,3 \
      | tr -d \" \
      | wget -qi- -O- \
      | ${__as_owner} tar zxf - -C ./.eth/ethdo/ \
      || echo "-> Could not download the latest version of '${REPO}' for amd64."
    ${__as_owner} mkdir -p ./.eth/ethdo/arm64
    wget -q -O- https://api.github.com/repos/${REPO}/releases/latest | grep "browser_download_url.*linux-arm64.tar.gz" \
      | head -1 \
      | cut -d : -f 2,3 \
      | tr -d \" \
      | wget -qi- -O- \
      | ${__as_owner} tar zxf - -C ./.eth/ethdo/arm64 \
      || echo "-> Could not download the latest version of '${REPO}' for arm64."
    ${__as_owner} mv ./.eth/ethdo/arm64/ethdo ./.eth/ethdo/ethdo-arm64
    ${__as_owner} rm -rf ./.eth/ethdo/arm64
    echo
    echo "Copy the contents of ./.eth/ethdo to a USB drive, and prepare a Linux Live USB to safely enter your mnemonic."
    echo "Please see https://ethdocker.com/Support/ChangingWithdrawalCredentials for details"
  elif [ "${1:-}" = "send-address-change" ]; then
    __i_haz_ethdo
    docompose run --rm ethdo validator credentials set
  elif [ "${1:-}" = "sign-exit" ] && [ "${2:-}" = "from-keystore" ]; then
    __i_haz_ethdo

    if echo "$@" | grep -q '.*--offline.*' 2>/dev/null ; then
      __offline="--offline"
    else
      __offline=""
    fi

    __non_interactive=0
    if echo "$@" | grep -q '.*--non-interactive.*' 2>/dev/null ; then
      __non_interactive=1
    fi
    if [ "${ETHD_FRONTEND:-}" = "noninteractive" ]; then
      __non_interactive=1
    fi

    if [ ${__non_interactive} = 1 ]; then
      __password="${KEYSTORE_PASSWORD}"
      __justone=1
    else
      __num_files=$(find .eth/validator_keys -maxdepth 1 -type f -name 'keystore*.json' | wc -l)
      if [ "$__num_files" -eq 0 ]; then
        echo "No keystore*.json files found in .eth/validator_keys/"
        echo "Nothing to do"
        exit 0
      fi

      if [ "$__num_files" -gt 1 ]; then
        while true; do
          read -rp "Do all validator keys have the same password? (y/n) " yn
          case $yn in
            [Yy]* ) __justone=1; break;;
            [Nn]* ) __justone=0; break;;
            * ) echo "Please answer yes or no.";;
          esac
        done
      else
        __justone=1
      fi
      if [ "${__justone}" -eq 1 ]; then
        while true; do
          read -srp "Please enter the password for your validator key(s): " __password
          echo
          read -srp "Please re-enter the password: " __password2
          echo
          if [ "${__password}" == "${__password2}" ]; then
            break
          else
            echo "The two entered passwords do not match, please try again."
            echo
          fi
        done
        echo
      fi
    fi

    created=0
    failed=0
    for __keyfile in .eth/validator_keys/keystore-*.json; do
      [ -f "${__keyfile}" ] || continue # Should always evaluate true - just in case
      if [ "${__justone}" -eq 0 ]; then
        while true; do
          read -srp "Please enter the password for your validator key stored in ${__keyfile}: " __password
          echo
          read -srp "Please re-enter the password: " __password2
          echo
          if [ "${__password}" == "${__password2}" ]; then
            break
          else
            echo "The two entered passwords do not match, please try again."
            echo
          fi
          echo
        done
      fi

      __pubkey="$(sed -E 's/.*"pubkey":\s*"([0-9a-fA-F]+)".*/\1/' < "${__keyfile}")"
      if [ -z "$__pubkey" ]; then
        echo "Unable to read public key from ${__keyfile}. Is it the right format?"
        continue
      else
          __pubkey="0x${__pubkey}"
      fi
      set +e
      # __offline may be empty, don't quote it
      # shellcheck disable=SC2086
      __json=$(docompose run --rm ethdo validator exit --validator "${__keyfile}" --json --timeout 2m \
        --passphrase "${__password}" ${__offline})
      exitstatus=$?
      if [ "${exitstatus}" -eq 0 ]; then
        echo "${__json}" >".eth/exit_messages/${__pubkey::10}--${__pubkey:90}-exit.json"
# shellcheck disable=SC2320
        exitstatus=$?
        if [ "${exitstatus}" -eq 0 ]; then
          echo "Creating an exit message for validator ${__pubkey} into file \
./.eth/exit_messages/${__pubkey::10}--${__pubkey:90}-exit.json succeeded"
          (( created++ ))
        else
          echo "Error writing exit json to file ./.eth/exit_messages/${__pubkey::10}--${__pubkey:90}-exit.json"
          (( failed++ ))
        fi
      else
        echo "Creating an exit message for validator ${__pubkey} from file ${__keyfile} failed"
        (( failed++ ))
      fi
      set -e
    done
    echo
    echo "Created pre-signed exit messages for ${created} validators"
    if [ "${created}" -gt 0 ]; then
      echo "You can find them in ./.eth/exit_messages"
    fi
    if [ "${failed}" -gt 0 ]; then
      echo "Failed for ${failed} validators"
    fi
  #elif [ "${1:-}" = "send-exit" ] && ! __i_haz_keys_service silent; then
  elif [ "${1:-}" = "send-exit" ]; then
    var="CL_NODE"
    CL_NODE=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    network_name="$(docompose config | awk '
      BEGIN {
          found_networks=0;
          found_default=0;
      }
      /networks:/ {
          found_networks=1;
          next;
      }
      found_networks && /default:/ {
          found_default=1;
          next;
      }
      found_default && /^ *name:/ {
          print $2;
          exit;
      }
      ')"
    if ! dodocker image ls --format "{{.Repository}}:{{.Tag}}" | grep -q "vc-utils:local"; then
      if ! dpkg-query -W -f='${Status}' docker-ce 2>/dev/null | grep -q "ok installed"; then
        dodocker build -t vc-utils:local ./vc-utils
      else
        if ! dpkg-query -W -f='${Status}' docker-buildx-plugin 2>/dev/null | grep -q "ok installed"; then
          ${__auto_sudo} apt-get update && ${__auto_sudo} apt-get install -y docker-buildx-plugin
        fi
        dodocker buildx build -t vc-utils:local ./vc-utils
      fi
    fi
    dodocker run --rm \
      -u 1000:1000 \
      --network "${network_name}" \
      --name send-exit \
      -v "$(pwd)/.eth/exit_messages:/exit_messages" \
      -v "/etc/localtime:/etc/localtime:ro" \
      -e "CL_NODE=${CL_NODE}" \
      --entrypoint "keymanager.sh" \
      vc-utils:local /var/lib/lighthouse/nonesuch.txt eth2 send-exit
  else
    #__i_haz_keys_service
    docompose run --rm -e OWNER_UID="${__owner_uid}" validator-keys "$@"
  fi
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
}


upgrade() {
  update
}


start() {
  docompose up -d --remove-orphans "$@"
}

# Passed by user
# shellcheck disable=SC2120
up() {
  start "$@"
}


run() {
  start "$@"
}


stop() {
  docompose down --remove-orphans "$@"
}


down() {
  stop "$@"
}


restart() {
  stop "$@"
  start "$@"
}


logs() {
  docompose logs "$@"
}


cmd() {
  docompose "$@"
}


terminate() {
  if [ -z "$(dodocker volume ls -q -f "name=^$(basename "$(realpath .)")_[^_]+")" ]; then
    echo "There are no data stores - Docker volumes - left to remove for this Ethereum node."
    stop
    return 0
  fi

  while true; do
    read -rp "WARNING - this action will destroy all data stores for this Ethereum node. Do you wish to continue? \
(No/Yes) " yn
    case $yn in
      [Yy][Ee][Ss] ) break;;
      * ) echo "Aborting, no changes made"; exit 130;;
    esac
  done

  stop
# In this case I want the word splitting, so rm can remove all volumes
# shellcheck disable=SC2046
  dodocker volume rm $(dodocker volume ls -q -f "name=^$(basename "$(realpath .)")_[^_]+")
  echo
  echo "All containers stopped and all volumes deleted"
  echo
}


query_network() {
  var="NETWORK"
  __prev_network=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  NETWORK=$(whiptail --notags --title "Select Network" --menu \
  "Which network do you want to run on?" 13 65 6 \
  "holesky" "Holeovice Testnet" \
  "mainnet" "Ethereum Mainnet" \
  "gnosis" "Gnosis Chain" \
  "sepolia" "Sepolia Testnet (permissioned validators)" \
  "custom" "Custom Testnet (needs a URL)" 3>&1 1>&2 2>&3)

  case "${NETWORK}" in
    "mainnet")
      echo "You chose to run on Ethereum mainnet"
      ;;
    "gnosis")
      echo "You chose to run on Gnosis Chain"
      ;;
    "sepolia" | "holesky" )
      echo "You chose to run on ${NETWORK} testnet"
      ;;
    "custom" )
      while true; do
        NETWORK=$(whiptail --title "Configure testnet URL" --inputbox "What is github URL of your custom testnet \
spec? (right-click to paste)" 10 60 3>&1 1>&2 2>&3)
        if [[ ${NETWORK} =~ ^https?:// ]]; then
          echo "Your custom testnet URL is: ${NETWORK}"
          break
        else
          whiptail --msgbox "${NETWORK} is not a valid URL. You can try again or Cancel on the next \
screen.\n\nCustom testnets only work with a URL to fetch their configuration from." 12 65
        fi
      done
      ;;
  esac
  if [ ! "${NETWORK}" = "${__prev_network}" ]; then
    __network_change=1
  else
    __network_change=0
  fi
}


query_deployment() {
  if [ "${NETWORK}" = "gnosis" ]; then
    if uname -m | grep -q riscv64; then
      echo "Gnosis network has no available client combos on RISC-V. Aborting."
      exit 1
    fi
    __deployment=$(whiptail --notags --title "Select deployment type" --menu \
    "What kind of deployment do you want to run?" 10 65 3 \
    "node" "Ethereum node - consensus, execution and validator client" \
    "rpc" "Ethereum RPC node - consensus and execution client" \
    "validator" "Validator client only" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
    __deployment=$(whiptail --notags --title "Select deployment type" --menu \
    "What kind of deployment do you want to run?" 10 65 4 \
    "node" "Ethereum node - consensus, execution and validator client" \
    "rpc" "Ethereum RPC node - consensus and execution client" \
    "lido_comp" "Lido-compatible node (Community Staking / Simple DVT)" \
    "rocket" "Validator client only - integrate with RocketPool" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q riscv64; then
    __deployment=$(whiptail --notags --title "Select deployment type" --menu \
    "What kind of deployment do you want to run?" 10 65 3 \
    "node" "Ethereum node - consensus, execution and validator client" \
    "rpc" "Ethereum RPC node - consensus and execution client" \
    "rocket" "Validator client only - integrate with RocketPool" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q x86_64; then
    __deployment=$(whiptail --notags --title "Select deployment type" --menu \
    "What kind of deployment do you want to run?" 11 65 5 \
    "node" "Ethereum node - consensus, execution and validator client" \
    "rpc" "Ethereum RPC node - consensus and execution client" \
    "lido_comp" "Lido-compatible node (Community Staking / Simple DVT)" \
    "rocket" "Validator client only - integrate with RocketPool" \
    "ssv" "SSV node - consensus, execution and ssv-node" 3>&1 1>&2 2>&3)
  else
    echo "Eth Docker does not recognize this CPU architecture. Aborting."
    echo "Output of uname -m"
    uname -m
    exit 1
  fi

  if [ "${__deployment}" = "lido_comp" ]; then
    __deployment=$(whiptail --notags --title "Select deployment type for Lido" --menu \
    "What kind of deployment to participate in Lido protocol do you want to run?" 13 90 3 \
    "lido_csm" "[Community Staking] CSM node - Consensus, execution and validator client" \
    "lido_ssv" "[Simple DVT] SSV node - Consensus, execution and ssv-node" \
    "lido_obol" "[Simple DVT] Obol node - Nodes, validator client and charon node (obol middleware)" 3>&1 1>&2 2>&3)
  fi

  echo "Your deployment choice is: ${__deployment}"
}


query_validator_client() {
  if [ "${NETWORK}" = "gnosis" ]; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select validator client" --menu \
    "Which validator client do you want to run?" 11 65 4 \
    "lighthouse-vc-only.yml" "Lighthouse validator client" \
    "teku-vc-only.yml" "Teku validator client" \
    "lodestar-vc-only.yml" "Lodestar validator client" \
    "nimbus-vc-only.yml" "Nimbus validator client" 3>&1 1>&2 2>&3)
  elif [ "${__deployment}" = "rocket" ]; then
    if uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
      CONSENSUS_CLIENT=$(whiptail --notags --title "Select validator client" --menu \
      "Which validator client do you want to run?" 11 65 4 \
      "lighthouse-vc-only.yml" "Lighthouse validator client" \
      "teku-vc-only.yml" "Teku validator client" \
      "lodestar-vc-only.yml" "Lodestar validator client" \
      "nimbus-vc-only.yml" "Nimbus validator client" 3>&1 1>&2 2>&3)
    else
      CONSENSUS_CLIENT=$(whiptail --notags --title "Select validator client" --menu \
      "Which validator client do you want to run?" 11 65 4 \
      "teku-vc-only.yml" "Teku validator client" \
      "lighthouse-vc-only.yml" "Lighthouse validator client" \
      "lodestar-vc-only.yml" "Lodestar validator client" \
      "nimbus-vc-only.yml" "Nimbus validator client" 3>&1 1>&2 2>&3)
    fi
  elif uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select validator client" --menu \
    "Which validator client do you want to run?" 11 65 4 \
    "lighthouse-vc-only.yml" "Lighthouse validator client" \
    "teku-vc-only.yml" "Teku validator client" \
    "lodestar-vc-only.yml" "Lodestar validator client" \
    "nimbus-vc-only.yml" "Nimbus validator client" 3>&1 1>&2 2>&3)
  else
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select validator client" --menu \
    "Which validator client do you want to run?" 12 65 5 \
    "teku-vc-only.yml" "Teku validator client" \
    "lighthouse-vc-only.yml" "Lighthouse validator client" \
    "lodestar-vc-only.yml" "Lodestar validator client" \
    "nimbus-vc-only.yml" "Nimbus validator client" \
    "prysm-vc-only.yml" "Prysm validator client" 3>&1 1>&2 2>&3)
  fi

  echo "Your validator client file is:" "${CONSENSUS_CLIENT}"
}


query_consensus_client() {
  if [ "${NETWORK}" = "gnosis" ]; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "teku.yml" "Teku (Java) - consensus and validator client" \
    "lighthouse.yml" "Lighthouse (Rust) - consensus and validator client" \
    "lodestar.yml" "Lodestar (Javascript) - consensus and validator client" \
    "nimbus.yml" "Nimbus (Nim) - consensus and validator client" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "nimbus.yml" "Nimbus (Nim) - consensus and validator client" \
    "grandine-allin1.yml" "Grandine (Rust) - consensus with built-in validator client" \
    "lodestar.yml" "Lodestar (Javascript) - consensus and validator client" \
    "teku.yml" "Teku (Java) - consensus and validator client" \
    "lighthouse.yml" "Lighthouse (Rust) - consensus and validator client" \
     3>&1 1>&2 2>&3)
  elif uname -m | grep -q riscv64; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "nimbus.yml" "Nimbus (Nim) - consensus and validator client" 3>&1 1>&2 2>&3)
  else
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 13 65 6 \
    "teku.yml" "Teku (Java) - consensus and validator client" \
    "grandine-allin1.yml" "Grandine (Rust) - consensus with built-in validator client" \
    "lodestar.yml" "Lodestar (Javascript) - consensus and validator client" \
    "nimbus.yml" "Nimbus (Nim) - consensus and validator client" \
    "lighthouse.yml" "Lighthouse (Rust) - consensus and validator client" \
    "prysm.yml" "Prysm (Go) - consensus and validator client" \
    3>&1 1>&2 2>&3)
  fi

  echo "Your consensus client file is:" "${CONSENSUS_CLIENT}"
}


query_consensus_only_client() {
  if [ "${NETWORK}" = "gnosis" ]; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "lighthouse-cl-only.yml" "Lighthouse (Rust) - consensus client" \
    "teku-cl-only.yml" "Teku (Java) - consensus client" \
    "lodestar-cl-only.yml" "Lodestar (Javascript) - consensus client" \
    "nimbus-cl-only.yml" "Nimbus (Nim) - consensus client" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "nimbus-cl-only.yml" "Nimbus (Nim) - consensus client" \
    "grandine-cl-only.yml" "Grandine (Rust) - consensus client" \
    "lodestar-cl-only.yml" "Lodestar (Javascript) - consensus client" \
    "lighthouse-cl-only.yml" "Lighthouse (Rust) - consensus client" \
    "teku-cl-only.yml" "Teku (Java) - consensus client" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q riscv64; then
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 11 65 4 \
    "nimbus-cl-only.yml" "Nimbus (Nim) - consensus client" 3>&1 1>&2 2>&3)
  else
    CONSENSUS_CLIENT=$(whiptail --notags --title "Select consensus client" --menu \
    "Which consensus client do you want to run?" 13 65 6 \
    "teku-cl-only.yml" "Teku (Java) - consensus client" \
    "grandine-cl-only.yml" "Grandine (Rust) - consensus client" \
    "lighthouse-cl-only.yml" "Lighthouse (Rust) - consensus client" \
    "nimbus-cl-only.yml" "Nimbus (Nim) - consensus client" \
    "lodestar-cl-only.yml" "Lodestar (Javascript) - consensus client" \
    "prysm-cl-only.yml" "Prysm (Go) - consensus client" 3>&1 1>&2 2>&3)
  fi

  echo "Your consensus client file is:" "${CONSENSUS_CLIENT}"
}


query_custom_execution_client() {
  if [ "${__minty_fresh}" -eq 1 ]; then
    EL_CUSTOM_NODE=""
    JWT_SECRET=""
  else
    var="EL_NODE"
    EL_CUSTOM_NODE=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    var="JWT_SECRET"
    JWT_SECRET=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  fi
  EL_CUSTOM_NODE=$(whiptail --title "Configure custom execution client" --inputbox "What is the URL for your custom \
execution client? (right-click to paste)" 10 65 "${EL_CUSTOM_NODE}" 3>&1 1>&2 2>&3)

  echo "Your custom execution client is: $EL_CUSTOM_NODE"

  while true; do
    JWT_SECRET=$(whiptail --title "Configure JWT secret" --inputbox "What is the JWT secret shared with the \
execution client? (right-click to paste)" 10 60 "${JWT_SECRET}" 3>&1 1>&2 2>&3)

    if [[ ${#JWT_SECRET} -eq 64 ]]; then
      echo "JWT Secret set. Please make sure it matches on CL and EL."
      break
    else
      whiptail --msgbox "The JWT secret needs to be exactly 32 bytes, 64 hex characters long. You can try \
again or Cancel on the next screen." 10 65
    fi
  done
}


query_execution_client() {
  if [ "${NETWORK}" = "gnosis" ]; then
    if uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
      EXECUTION_CLIENT=$(whiptail --notags --title "Select execution client" --menu \
      "Which execution client do you want to run?" 9 65 2 \
      "nethermind.yml" "Nethermind (.NET)" \
      "NONE" "Custom - Distributed" 3>&1 1>&2 2>&3)
    else
      EXECUTION_CLIENT=$(whiptail --notags --title "Select execution client" --menu \
      "Which execution client do you want to run?" 10 65 3 \
      "nethermind.yml" "Nethermind (.NET)" \
      "erigon.yml" "Erigon (Go)" \
      "NONE" "Custom - Distributed" 3>&1 1>&2 2>&3)
    fi
  elif uname -m | grep -q aarch64 || uname -m | grep -q arm64; then
    EXECUTION_CLIENT=$(whiptail --notags --title "Select execution client" --menu \
    "Which execution client do you want to run?" 11 65 4 \
    "besu.yml" "Besu (Java)" \
    "nethermind.yml" "Nethermind (.NET)" \
    "geth.yml" "Geth (Go)" \
    "NONE" "Custom - Distributed" 3>&1 1>&2 2>&3)
  elif uname -m | grep -q riscv64; then
    EXECUTION_CLIENT=$(whiptail --notags --title "Select execution client" --menu \
    "Which execution client do you want to run?" 11 65 4 \
    "geth.yml" "Geth (Go)" \
    "NONE" "Custom - Distributed" 3>&1 1>&2 2>&3)
  else
    EXECUTION_CLIENT=$(whiptail --notags --title "Select execution client" --menu \
    "Which execution client do you want to run?" 13 65 6 \
    "reth.yml" "Reth (Rust)" \
    "besu.yml" "Besu (Java)" \
    "nethermind.yml" "Nethermind (.NET)" \
    "erigon.yml" "Erigon (Go)" \
    "geth.yml" "Geth (Go)" \
    "NONE" "Custom - Distributed" 3>&1 1>&2 2>&3)
  fi

  if [ "${EXECUTION_CLIENT}" == "NONE" ]; then
    unset EXECUTION_CLIENT
    query_custom_execution_client
    EL_NODE="${EL_CUSTOM_NODE}"
  else
    echo "Your execution client file is:" "${EXECUTION_CLIENT}"
# This gets used, but shellcheck doesn't recognize that
# shellcheck disable=SC2034
    EL_NODE="http://execution:8551"
    if [ "${EXECUTION_CLIENT}" = "erigon.yml" ]; then
        echo "Please remember to set your EL_WS_PORT to match EL_RPC_PORT for Erigon"
    fi
  fi
}


query_grafana() {
  if (whiptail --title "Grafana" --yesno "Do you want to use Grafana dashboards?" 10 65) then
      if [[ "$OSTYPE" == "darwin"* ]]; then
      # macOS doesn't do well with / bind mount - leave node-exporter, cadvisor and loki/promtail off by default
        GRAFANA_CLIENT="grafana-rootless.yml:grafana-shared.yml"
      else
        GRAFANA_CLIENT="grafana.yml:grafana-shared.yml"
      fi
  else
    unset GRAFANA_CLIENT
  fi
}


query_remote_beacon() {
  if [ "${__minty_fresh}" -eq 1 ]; then
    if [ "${__deployment}" = "rocket" ]; then
      REMOTE_BEACON="http://eth2:5052"
    else
      REMOTE_BEACON=""
    fi
  else
    var="CL_NODE"
    REMOTE_BEACON=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  fi
  REMOTE_BEACON=$(whiptail --title "Configure remote consensus client" --inputbox "What is the URL for your remote \
consensus client? (right-click to paste)" 10 60 "${REMOTE_BEACON}" 3>&1 1>&2 2>&3)

  echo "Your remote consensus client is:" "${REMOTE_BEACON}"
}


query_checkpoint_beacon() {
  if [ "${__minty_fresh}" -eq 1 ] || [ "${__network_change}" -eq 1 ]; then
    RAPID_SYNC_URL=""
  else
    var="RAPID_SYNC_URL"
    RAPID_SYNC_URL=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  fi
  if [ -z "${RAPID_SYNC_URL}" ]; then
    case "${NETWORK}" in
      "sepolia")
        RAPID_SYNC_URL="https://sepolia.beaconstate.info"
        ;;
      "holesky")
        RAPID_SYNC_URL="https://holesky.beaconstate.info"
        ;;
      "mainnet")
        RAPID_SYNC_URL="https://beaconstate.info"
        ;;
      "gnosis")
        RAPID_SYNC_URL="https://checkpoint.gnosischain.com"
        ;;
      *)
        RAPID_SYNC_URL=""
        ;;
    esac
  fi

  RAPID_SYNC_URL=$(whiptail --title "Configure CL checkpoint sync URL" --inputbox "What is the URL for your CL \
checkpoint sync provider? (right-click to paste)" 10 65 "${RAPID_SYNC_URL}" 3>&1 1>&2 2>&3)

  echo "Your checkpoint sync URL is:" "${RAPID_SYNC_URL}"
}


query_graffiti() {
  var="GRAFFITI"
  GRAFFITI=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

  while true; do
    GRAFFITI=$(whiptail --title "Configure Graffiti" --inputbox "What Graffiti do you want to send with your blocks? \
(up to 32 characters)" 10 65 "${GRAFFITI}" 3>&1 1>&2 2>&3)

    if [[ $(echo -n "${GRAFFITI}" | wc -c) -gt 32 ]]; then
      whiptail --msgbox "The graffiti string cannot be longer than 32 characters. Emojis count as 4, each." 16 65
    else
      break
    fi
  done

  echo "your Graffiti is:" "${GRAFFITI}"
}


query_rapid_sync() {
  if [[ "${NETWORK}" =~ ^https?:// ]]; then
    RAPID_SYNC_URL=""
    return
  fi
  query_checkpoint_beacon
}


query_coinbase() {
  var="FEE_RECIPIENT"
  FEE_RECIPIENT=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

    while true; do
        set +e # Can't rely on the error handler here because of the special-casing below for update()
        if [[ "${__deployment}" =~ "lido_" ]]; then
            case "${NETWORK}" in
                "mainnet")
                    FEE_RECIPIENT="0x388c818ca8b9251b393131c08a736a67ccb19297"
                    ;;
                "holesky")
                    FEE_RECIPIENT="0xE73a3602b99f1f913e72F8bdcBC235e206794Ac8"
                    ;;
                *)
                    FEE_RECIPIENT="0x0000000000000000000000000000000000000000"
                    ;;
            esac
        elif [ "${__during_update}" -eq 1 ] || [ ! "${__deployment}" = rpc ]; then
            FEE_RECIPIENT=$(whiptail --title "Configure rewards address" --inputbox "What is the address you want \
transaction rewards to be sent to by default? (right-click to paste, CANNOT be an ENS)" 10 65 "${FEE_RECIPIENT}" \
3>&1 1>&2 2>&3)
    else
      FEE_RECIPIENT=$(whiptail --title "Configure fee recipient" --inputbox "What is the fallback fee recipient \
address? Yes even on an RPC node. Can be any address at all. (right-click to paste, CANNOT be an ENS)" 10 65 \
"${FEE_RECIPIENT}" 3>&1 1>&2 2>&3)
    fi

    exitstatus=$?
    set -e
    if [ $exitstatus -eq 0 ]; then
      if [[ ${FEE_RECIPIENT} == 0x* && ${#FEE_RECIPIENT} -eq 42 ]]; then
        echo "Your rewards address is: ${FEE_RECIPIENT}"
        break
      else
        whiptail --msgbox "${FEE_RECIPIENT} is not a valid ETH address. You can try again or Cancel on the next \
screen.\n\nThe client will not start successfully until a valid ETH rewards address has been set." 16 65
      fi
    else
      if [ $__during_update -eq 1 ]; then
        echo
        echo "Please make requested changes manually or run \"$__me update\" again"
        echo "before running \"$__me up\"."
        echo
        echo "Without a FEE_RECIPIENT set in \"${ENV_FILE}\", containers will not"
        echo "start successfully. Already running containers will keep running with the"
        echo "old configuration until you are ready to restart them."
      else
        echo "Canceled config wizard."
      fi
      echo
      exit 130
    fi
  done
}


query_mev() {
  if [ "${NETWORK}" = "gnosis" ]; then
    return 0
  fi
  if [ "${__deployment}" = "ssv" ]; then
    MEV_BOOST="true"
    case "${NETWORK}" in
      "holesky")
        MEV_RELAYS="https://0xafa4c6985aa049fb79dd37010438cfebeb0f2bd42b115b89dd678dab0670c1de38da0c4e9138c9290a398ecd9a0b3110@boost-relay-holesky.flashbots.net,\
https://0xaa58208899c6105603b74396734a6263cc7d947f444f396a90f7b7d3e65d102aec7e5e5291b27e08d02c50a050825c2f@holesky.titanrelay.xyz,\
https://0xb1d229d9c21298a87846c7022ebeef277dfc321fe674fa45312e20b5b6c400bfde9383f801848d7837ed5fc449083a12@relay-holesky.edennetwork.io,\
https://0x821f2a65afb70e7f2e820a925a9b4c80a159620582c1766b1b09729fec178b11ea22abb3a51f07b288be815a1a2ff516@bloxroute.holesky.blxrbdn.com,\
https://0xb1559beef7b5ba3127485bbbb090362d9f497ba64e177ee2c8e7db74746306efad687f2cf8574e38d70067d40ef136dc@relay-stag.ultrasound.money"
        ;;
      "mainnet")
        MEV_RELAYS="https://0xac6e77dfe25ecd6110b8e780608cce0dab71fdd5ebea22a16c0205200f2f8e2e3ad3b71d3499c54ad14d6c21b41a37ae@boost-relay.flashbots.net,\
https://0x8b5d2e73e2a3a55c6c87b8b6eb92e0149a125c852751db1422fa951e42a09b82c142c3ea98d0d9930b056a3bc9896b8f@bloxroute.max-profit.blxrbdn.com,\
https://0xb3ee7afcf27f1f1259ac1787876318c6584ee353097a50ed84f51a1f21a323b3736f271a895c7ce918c038e4265918be@relay.edennetwork.io,\
https://0xa1559ace749633b997cb3fdacffb890aeebdb0f5a3b6aaa7eeeaf1a38af0a8fe88b9e4b1f61f236d2e64d95733327a62@relay.ultrasound.money,\
https://0xa15b52576bcbf1072f4a011c0f99f9fb6c66f3e1ff321f11f461d15e31b1cb359caa092c71bbded0bae5b5ea401aab7e@aestus.live,\
https://0x98650451ba02064f7b000f5768cf0cf4d4e492317d82871bdc87ef841a0743f69f0f1eea11168503240ac35d101c9135@mainnet-relay.securerpc.com"
        ;;
    esac
    return 0
  fi
  var="MEV_BOOST"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  # I do mean to match literally
  # shellcheck disable=SC2076
  if [[ "${CONSENSUS_CLIENT}" =~ "-vc-only.yml" ]]; then
    if (whiptail --title "MEV Boost" --yesno "Is MEV Boost configured on your remote consensus client and do you \
want to use MEV Boost?" 10 65); then
        MEV_BOOST="true"
        MEV_RELAYS=""
      fi
      return 0
  fi
  if [[ "${__deployment}" =~ "lido_" ]]; then
    MEV_BOOST="true"
    while true; do
      MEV_RELAYS=""
      __selected=""
      declare -A relays=()
      declare -A optional_relays=()
      case "${NETWORK}" in
          "mainnet")
              relays=(
                  ['Agnostic']="https://0xa7ab7a996c8584251c8f925da3170bdfd6ebc75d50f5ddc4050a6fdc77f2a3b5fce2cc750d0865e05d7228af97d69561@agnostic-relay.net"
                  ['bloXroute']="https://0xb0b07cd0abef743db4260b0ed50619cf6ad4d82064cb4fbec9d3ec530f7c5e6793d9f286c4e082c0244ffb9f2658fe88@bloxroute.regulated.blxrbdn.com"
                  ['Aestus']="https://0xa15b52576bcbf1072f4a011c0f99f9fb6c66f3e1ff321f11f461d15e31b1cb359caa092c71bbded0bae5b5ea401aab7e@aestus.live"
                  ['bloXroute Max-Profit']="https://0x8b5d2e73e2a3a55c6c87b8b6eb92e0149a125c852751db1422fa951e42a09b82c142c3ea98d0d9930b056a3bc9896b8f@bloxroute.max-profit.blxrbdn.com"
                  ['Flashbots']="https://0xac6e77dfe25ecd6110b8e780608cce0dab71fdd5ebea22a16c0205200f2f8e2e3ad3b71d3499c54ad14d6c21b41a37ae@boost-relay.flashbots.net"
                  ['Eden Network']="https://0xb3ee7afcf27f1f1259ac1787876318c6584ee353097a50ed84f51a1f21a323b3736f271a895c7ce918c038e4265918be@relay.edennetwork.io"
                  ['Ultra Sound']="https://0xa1559ace749633b997cb3fdacffb890aeebdb0f5a3b6aaa7eeeaf1a38af0a8fe88b9e4b1f61f236d2e64d95733327a62@relay.ultrasound.money"
              )
              optional_relays=(
                  ['Manifold Finance']="https://0x98650451ba02064f7b000f5768cf0cf4d4e492317d82871bdc87ef841a0743f69f0f1eea11168503240ac35d101c9135@mainnet-relay.securerpc.com/"
              )
              __selected=$(whiptail --title "Relays list" --checklist \
                  "Choose relays" 15 50 9 \
                  "Agnostic" "" ON \
                  "bloXroute" "" ON \
                  "Aestus" "" ON \
                  "bloXroute Max-Profit" "" ON \
                  "Flashbots" "" ON \
                  "Eden Network" "" ON \
                  "Manifold Finance" "(optional)" ON \
                  "Ultra Sound" "" ON 3>&1 1>&2 2>&3)
              ;;
          "holesky")
              relays=(
                  ['Aestus']="https://0xab78bf8c781c58078c3beb5710c57940874dd96aef2835e7742c866b4c7c0406754376c2c8285a36c630346aa5c5f833@holesky.aestus.live"
                  ['Titan']="https://0xaa58208899c6105603b74396734a6263cc7d947f444f396a90f7b7d3e65d102aec7e5e5291b27e08d02c50a050825c2f@holesky.titanrelay.xyz"
                  ['Flashbots Boost']="https://0xafa4c6985aa049fb79dd37010438cfebeb0f2bd42b115b89dd678dab0670c1de38da0c4e9138c9290a398ecd9a0b3110@boost-relay-holesky.flashbots.net"
                  ['Ultrasound']="https://0xb1559beef7b5ba3127485bbbb090362d9f497ba64e177ee2c8e7db74746306efad687f2cf8574e38d70067d40ef136dc@relay-stag.ultrasound.money"
              )
              __selected=$(whiptail --title "Relays list" --checklist \
                  "Choose relays" 12 30 5 \
                  "Aestus" "" ON \
                  "Titan" "" ON \
                  "Flashbots Boost" "" ON \
                  "Ultrasound" "" ON 3>&1 1>&2 2>&3)
              ;;
            *)
              echo "No MEV RELAYS configured for ${NETWORK}"
              return
              ;;
      esac
      for i in "${!relays[@]}"; do
          if [[ ${__selected} =~ ${i} ]]; then
              if [ -z "${MEV_RELAYS}" ]; then
                  MEV_RELAYS="${relays[$i]}"
              else
                  MEV_RELAYS="${MEV_RELAYS},${relays[$i]}"
              fi
          fi
      done
      exitstatus=$?
      if [ $exitstatus -eq 0 ]; then
          if [ -z "${MEV_RELAYS}" ]; then
            whiptail --msgbox "At least one mandatory relay should be chosen" 10 75
            continue
          fi
      else
          echo "You chose Cancel."
          exit 1
      fi
      for i in "${!optional_relays[@]}"; do
          if [[ ${__selected} =~ ${i} ]]; then
              if [ -z "${MEV_RELAYS}" ]; then
                  MEV_RELAYS="${optional_relays[$i]}"
              else
                  MEV_RELAYS="${MEV_RELAYS},${optional_relays[$i]}"
              fi
          fi
      done
      break
    done
    return 0
  fi
  if (whiptail --title "MEV Boost" --yesno "Do you want to use MEV Boost?" 10 65) then
    MEV_BOOST="true"
    if [ "${value}" = "true" ]; then
      var="MEV_RELAYS"
      MEV_RELAYS=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
    else
      case "${NETWORK}" in
        "sepolia")
          MEV_RELAYS=https://0x845bd072b7cd566f02faeb0a4033ce9399e42839ced64e8b2adcfc859ed1e8e1a5a293336a49feac6d9a5edb779be53a@boost-relay-sepolia.flashbots.net
          ;;
        "holesky")
          MEV_RELAYS="https://0xafa4c6985aa049fb79dd37010438cfebeb0f2bd42b115b89dd678dab0670c1de38da0c4e9138c9290a398ecd9a0b3110@boost-relay-holesky.flashbots.net,\
https://0xaa58208899c6105603b74396734a6263cc7d947f444f396a90f7b7d3e65d102aec7e5e5291b27e08d02c50a050825c2f@holesky.titanrelay.xyz,\
https://0xb1d229d9c21298a87846c7022ebeef277dfc321fe674fa45312e20b5b6c400bfde9383f801848d7837ed5fc449083a12@relay-holesky.edennetwork.io,\
https://0x821f2a65afb70e7f2e820a925a9b4c80a159620582c1766b1b09729fec178b11ea22abb3a51f07b288be815a1a2ff516@bloxroute.holesky.blxrbdn.com,\
https://0xb1559beef7b5ba3127485bbbb090362d9f497ba64e177ee2c8e7db74746306efad687f2cf8574e38d70067d40ef136dc@relay-stag.ultrasound.money"
          ;;
        "mainnet")
          MEV_RELAYS=https://0xac6e77dfe25ecd6110b8e780608cce0dab71fdd5ebea22a16c0205200f2f8e2e3ad3b71d3499c54ad14d6c21b41a37ae@boost-relay.flashbots.net,\
https://0x8b5d2e73e2a3a55c6c87b8b6eb92e0149a125c852751db1422fa951e42a09b82c142c3ea98d0d9930b056a3bc9896b8f@bloxroute.max-profit.blxrbdn.com,\
https://0xb3ee7afcf27f1f1259ac1787876318c6584ee353097a50ed84f51a1f21a323b3736f271a895c7ce918c038e4265918be@relay.edennetwork.io,\
https://0xa1559ace749633b997cb3fdacffb890aeebdb0f5a3b6aaa7eeeaf1a38af0a8fe88b9e4b1f61f236d2e64d95733327a62@relay.ultrasound.money,\
https://0xa15b52576bcbf1072f4a011c0f99f9fb6c66f3e1ff321f11f461d15e31b1cb359caa092c71bbded0bae5b5ea401aab7e@aestus.live,\
https://0xa7ab7a996c8584251c8f925da3170bdfd6ebc75d50f5ddc4050a6fdc77f2a3b5fce2cc750d0865e05d7228af97d69561@agnostic-relay.net,\
https://0x98650451ba02064f7b000f5768cf0cf4d4e492317d82871bdc87ef841a0743f69f0f1eea11168503240ac35d101c9135@mainnet-relay.securerpc.com,\
https://0x8c7d33605ecef85403f8b7289c8058f440cbb6bf72b055dfe2f3e2c6695b6a1ea5a9cd0eb3a7982927a463feb4c3dae2@relay.wenmerge.com
          ;;
        *)
          MEV_RELAYS=""
          ;;
      esac
    fi
    MEV_RELAYS=$(whiptail --title "Configure MEV relays" --inputbox "What MEV relay(s) do you want to use? \
(right-click to paste)" 10 65 "${MEV_RELAYS}" 3>&1 1>&2 2>&3)
    echo "Your MEV relay(s): ${MEV_RELAYS}"
  else
    MEV_BOOST="false"
    MEV_RELAYS=""
  fi
}

lido_withdrawal_credentials_address() {
    __lido_address=""
    case "${NETWORK}" in
        "mainnet")
            __lido_address="0xB9D7934878B5FB9610B3fE8A5e441e8fad7E293f"
            ;;
        "holesky")
            __lido_address="0xF0179dEC45a37423EAD4FaD5fCb136197872EAd9"
            ;;
        *)
            __lido_address="0x0000000000000000000000000000000000000000"
            ;;
    esac
    echo "${__lido_address}"
}

lido_keys_attention_message() {
  whiptail --title "Attention" --msgbox "Please, make sure that you set 32 ETH when generated deposit data\nAnd right execution address for your validator keys: $(lido_withdrawal_credentials_address)\nOtherwise, your keys will not be valid!" 10 80
}

query_lido_keys_generation() {
    if [ "${NETWORK}" = "mainnet" ]; then
      if (whiptail --title "Security warning" --yesno "Key generation is not recommended on MAINNET for security reasons.\n\nIt is recommended to Select 'No' to skip the step and generate keys in a more secure way later (ex. on an airgapped live USB)\n\nOtherwise, Select 'Yes' to proceed with key generation on this machine" 13 85) then
         echo "Proceeding with key generation on MAINNET."
      else
         lido_keys_attention_message
         return 0
      fi
    fi

    __num_validators="1"
    __keystore_password=""
    __keystore_password_confirm=""
    __num_validators=$(whiptail --title "Validators count" --inputbox "Enter the number of validators" 8 60 "${__num_validators}" 3>&1 1>&2 2>&3)
    while true; do
        __keystore_password=$(whiptail --title "Keystore password" --passwordbox "Enter validators keystore password (at least 8 chars)" 8 60 "${__keystore_password}" 3>&1 1>&2 2>&3)

        exitstatus=$?
        if [ $exitstatus -eq 0 ]; then
            if [[ ${#__keystore_password} -ge 8 ]]; then
              __keystore_password_confirm=$(whiptail --title "Keystore password" --passwordbox "Confirm validators keystore password" 8 60 "${__keystore_password_confirm}" 3>&1 1>&2 2>&3)
              if [ "${__keystore_password}" = "${__keystore_password_confirm}" ]; then
                  echo "Keystore password set."
                  break
              else
                  whiptail --msgbox "Passwords do not match. Please try again." 10 60
              fi
            else
              whiptail --msgbox "The keystore password secret needs to be at least 8 characters long. You can try \
again or Cancel on the next screen." 10 75
            fi
        else
            echo "You chose Cancel."
            exit 1
        fi
    done

    exitstatus=$?
    if [ $exitstatus -eq 0 ]; then
        echo "Your number of validators is:" "${__num_validators}"
        __mnemonic="existing"
        if (whiptail --title "Mnemonic" --yesno "Do you want to generate new mnemonic?" 8 60) then
            __mnemonic="new"
        fi
        export NETWORK=${NETWORK} && docompose --profile tools run --rm deposit-cli-${__mnemonic} \
            --uid "$(id -u)" \
            --execution_address "$(lido_withdrawal_credentials_address)" \
            --num_validators "${__num_validators}" \
            --keystore_password "${__keystore_password}" \
            --non_interactive
    else
        echo "You chose Cancel."
        exit 1
    fi
}


query_lido_obol_enr() {
    ${__as_owner} mkdir -p ./.eth
    __outcome__=$(docompose -f ./lido-obol.yml run -u "$(id -u)":"$(id -g)" --rm charon-create-enr)
    if [[ "${__outcome__}" =~ "Created ENR private key:" ]]; then
      __lido_obol_operator_enr=$(echo "${__outcome__}" | grep -e 'enr:')
    else
      echo "Something went wrong. Please, try again."
      exit 1
    fi

    echo "Your created ENR is:" "${__lido_obol_operator_enr}"
    echo "${__lido_obol_operator_enr}" >> "./.eth/charon-enr-public-key"
    whiptail --title "Lido Obol operator ENR creation outcome" --msgbox "Your ENR is created!\n\n1. Backup your private key (path: .eth/charon-enr-private-key)!\n2. Copy your public ENR for the futher steps\n\nYour public ENR is:\n\n${__lido_obol_operator_enr}" 16 80
}

query_lido_obol_cluster_definition() {
    __cluster_definition_url=$(whiptail --title "Lido Obol cluster creation" --inputbox "\nPut your cluster definition link below:" 10 80 "https://api.obol.tech/dv/example_link_to_your_definition" 3>&1 1>&2 2>&3)
    if [ "${__cluster_definition_url}" = ""  ]; then
      echo "Cluster definition URL can't be empty"
      exit 1
    fi
    exitstatus=$?
    if [ $exitstatus -eq 0 ]; then
        ${__as_owner} curl -o ./.eth/cluster_definition.tmp -s "${__cluster_definition_url}" -H "Accept: application/json"
# shellcheck disable=SC2086
        __cluster_definition_is_valid=$(docompose -f ./lido-obol.yml run --rm -v "$(pwd)"/.eth/cluster_definition.tmp:/cluster_definition.json:ro curl-jq sh -c \
          "cat /cluster_definition.json | jq -r 'all(.validators[]; .fee_recipient_address == \"'${FEE_RECIPIENT}'\" and .withdrawal_address == \"'$(lido_withdrawal_credentials_address)'\")'" | tail -n 1)
        set -e
        if [ "${__cluster_definition_is_valid}" = "true" ]; then
            echo "Your cluster definition url is:" "${__cluster_definition_url}"
            ${__as_owner} mv ./.eth/cluster_definition.tmp ./.eth/cluster-definition.json
        else
            whiptail --title "Lido Obol cluster creation" --msgbox "Your cluster definition is not valid.\n\nCheck that every validator has \`fee_recipient_address\` and \`withdrawal_address\` equal to Lido contracts and try again.\n\nLido fee recipient: '${FEE_RECIPIENT}'\nLido withdrawal credentials: '$(lido_withdrawal_credentials_address)'" 14 90
            echo "Your cluster definition is NOT valid."
            ${__as_owner} rm ./.eth/cluster_definition.tmp
            exit 1
        fi
    else
        echo "You chose Cancel."
        exit 1
    fi
}

query_lido_obol_cluster_dkg() {
    if [ -d ./.eth/validator_keys ]; then
        __folder_postfix=${EPOCHSECONDS}
        ${__as_owner} mkdir ./.eth_backup_"$__folder_postfix"
        ${__as_owner} cp -vr ./.eth/validator_keys ./.eth_backup_"$__folder_postfix"/validator_keys
        ${__as_owner} rm -rf ./.eth/validator_keys
    fi
    if (whiptail --title "DKG ceremony" --yesno "Do you want to start DKG ceremony?\n\nMake sure all participants are ready!" 10 60) then
        __outcome__=$(docompose -f ./lido-obol.yml run -u "$(id -u)":"$(id -g)" --rm charon-run-dkg)
        exitstatus=$?
        if [ $exitstatus -ne 0 ]; then
            echo "Something went wrong. Please, try again."
            exit 1
        fi
        echo "DKG ceremony finished successfully"
        whiptail --title "Finish" --msgbox "\nThe DKG is finished!" 10 40
    else
        whiptail --title "DKG ceremony" --msgbox "You should start DKG ceremony before proceeding further" 8 60
        echo "DKG ceremony starting is canceled"
        exit 1
    fi
}

query_dkg() {
  __ssv_operator_id=-1
  if (whiptail --title "DKG ceremony" --yesno "Do you want to participate in DKG ceremonies as an operator?" 10 60); then
    __key_file_content=$(${__auto_sudo} cat ./ssv-config/encrypted_private_key.json)
    __public_key=$(docompose -f ./ssv-dkg.yml run --rm curl-jq sh -c \
      "echo '${__key_file_content}' | jq -r '.pubKey'" | tail -n 1)
    echo "Your SSV node public key is: ${__public_key}"
    __ssv_operator_id=$(whiptail --title "Register SSV operator" --inputbox "\n1. Your SSV node public key:\n\n${__public_key}\n\n2. Register your operator in the SSV network with the public key\n\n3. Input your Operator ID \
(right-click to paste)" 22 85 3>&1 1>&2 2>&3)
    if [[ -n "${__ssv_operator_id}" && ! "${__ssv_operator_id}" = "-1" ]]; then
      sed -i'.original' "s|operatorID: .*|operatorID: ${__ssv_operator_id}|" ./ssv-config/dkg-config.yaml
      echo "Your SSV Operator ID is: ${__ssv_operator_id}"
    else
      echo "Please manually edit \"./ssv-config/dkg-config.yaml\" with your SSV Operator ID"
      echo "and add \":ssv-dkg.yml\" to \"COMPOSE_FILE\" in \".env\" after registering your operator."
    fi
  fi
  rm -f ssv-config/dkg-config.yaml.original
}

set_value_in_env() {
# Assumes that "var" has been set to the name of the variable to be changed
  if [ "${!var+x}" ]; then
    if ! grep -qF "${var}" "${ENV_FILE}" 2>/dev/null ; then
      echo "${var}=${!var}" >> "${ENV_FILE}"
    else
# Handle & in GRAFFITI gracefully
      sed -i'.original' -e "s~^\(${var}\s*=\s*\).*\$~\1${!var//&/\\&}~" "${ENV_FILE}"
    fi
  fi
}


handle_error() {
  if [[ ! $- =~ e ]]; then
# set +e, do nothing
    return 0
  fi

  local exit_code=$1
  echo
  if [ "$exit_code" -eq 130 ]; then
    echo "$__me terminated by user"
  elif [ "$__during_config" -eq 1 ] && [ "$exit_code" -eq 1 ]; then
    echo "Canceled config wizard."
  else
    echo "$__me terminated with exit code $exit_code on line $2"
    if [ -n "${__command}" ]; then
      echo "This happened during $__me ${__command} ${__params}"
    fi
  fi
  if [ "$__during_update" -eq 1 ] && [ "$__during_migrate" -eq 1 ]; then
    cp "${ENV_FILE}" "${ENV_FILE}".partial
    cp "${ENV_FILE}".source "${ENV_FILE}"
    echo
    echo "Restored your ${ENV_FILE} file, to undo partial migration. Please verify it looks correct."
    echo "The partially migrated file is in ${ENV_FILE}.partial for troubleshooting."
  fi
  if [ "$__during_postgres" -eq 1 ]; then
    echo
    if [ "$__during_migrate" -eq 1 ] && [ "$__migrated" -eq 0 ]; then
      echo "Web3signer slashing protection database migration failed, while switching to the migrated data."
      echo
      echo "WARNING: You are no longer protected by the slashing protection database."
      echo "Starting the node again could get you slashed."
      echo
      echo "Marking Web3signer as unsafe to start."
      dodocker run --rm -v "$(dodocker volume ls -q -f "name=web3signer-keys")":/var/lib/web3signer \
        alpine:3 touch /var/lib/web3signer/.migration_fatal_error
    elif [ "$__migrated" -eq 1 ]; then
      echo "Web3signer slashing protection database migration failed, after switching to the migrated data."
      echo
      echo "The slashing protection database itself is likely fine, but somewhere in the switch to PostgreSQL 16"
      echo "an error occurred, which is likely to keep your node from functioning correctly."
    else
      echo "Web3signer slashing protection database migration failed, but before removing the original data."
      echo
      echo "Your original Web3signer slashing protection database remains in place."
      echo "You can safely update the node again, this time without migration, and start it."
    fi
  fi
}


check_legacy() {
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)

# Literal match intended
# shellcheck disable=SC2076
  if [[ "${value}" =~ "-allin1.yml" && ! "${value}" =~ "grandine-allin1.yml" ]]; then # Warn re Grandine once VC
    if [[ "${value}" =~ "teku-allin1.yml" ]]; then
      __client="Teku"
    elif [[ "${value}" =~ "nimbus-allin1.yml" ]]; then
      __client="Nimbus"
    elif [[ "${value}" =~ "grandine-allin1.yml" ]]; then
      __client="Grandine"
    else
      __client="Mystery"
    fi
    if ! (whiptail --title "All-In-One detected" --yesno "All-In-One client ${__client} detected. Re-configuration requires re-import of the keys, which has to be treated like a move with 15 minutes downtime, to avoid slashing. Do you wish to continue, regardless?" 10 65 --defaultno) then
      echo "Aborting config"
      exit 0
    fi
  fi
}

config() {
  # Do not track changes to ext-network.yml
  ${__as_owner} git update-index --assume-unchanged ext-network.yml
  # Create ENV file if needed
  if ! [[ -f "${ENV_FILE}" ]]; then
    ${__as_owner} cp default.env "${ENV_FILE}"
    __minty_fresh=1
  else
    __minty_fresh=0
  fi

  __during_config=1

  check_legacy
  query_network
  query_deployment
  case "${__deployment}" in
    "node" | "lido_csm")
        query_consensus_client
        ;;
    "lido_obol")
      query_consensus_client
      ;;
    "validator" | "rocket")
      query_validator_client
      ;;
    "ssv" | "lido_ssv")
      if [ "${NETWORK}" = "holesky" ]; then
        sed -i'.original' 's/  Network: .*/  Network: holesky/' ssv-config/config.yaml
      elif [ "${NETWORK}" = "mainnet" ]; then
        sed -i'.original' 's/  Network: .*/  Network: mainnet/' ssv-config/config.yaml
      else
        echo "${NETWORK} is not something that works with SSV."
        echo "Please choose Holeovice or Mainnet when running $__me config again"
        echo "Aborting."
        exit 1
      fi
      rm ssv-config/config.yaml.original
      if [ ! -f "./ssv-config/password.pass" ]; then
        echo "Creating password file for encrypted SSV secret key"
        head -c 16 /dev/urandom | base64 | tr -d '[:space:]' >./ssv-config/password.pass
        ${__auto_sudo} chown 12000:12000 ./ssv-config/password.pass
        ${__auto_sudo} chmod 600 ./ssv-config/password.pass
      fi
      if [ ! -f "./ssv-config/encrypted_private_key.json" ]; then
        echo "Creating encrypted operator private key"
        dodocker run --name ssv-node-key-generation -v "$(pwd)/ssv-config/password.pass":/password.pass \
          -it bloxstaking/ssv-node:latest /go/bin/ssvnode generate-operator-keys \
          --password-file=/password.pass && dodocker cp ssv-node-key-generation:/encrypted_private_key.json \
          ./ssv-config/encrypted_private_key.json && dodocker rm ssv-node-key-generation
        ${__auto_sudo} chown 12000:12000 ./ssv-config/encrypted_private_key.json
      fi
      query_dkg
      query_consensus_only_client
      ;;
    "rpc")
      query_consensus_only_client
      ;;
    *)
      echo "Unknown deployment ${__deployment}, this is a bug."
      exit 70
      ;;
  esac

  MEV_BOOST=false
# I do mean to match literally
# shellcheck disable=SC2076
  if [[ ! "${CONSENSUS_CLIENT}" =~ "-vc-only.yml" ]]; then
    CL_NODE="http://consensus:5052"

    query_execution_client
    query_rapid_sync
    query_mev
    query_grafana
    query_coinbase
    if [[ "${__deployment}" = "node" || "${__deployment}" = "lido_csm" ]]; then
      query_graffiti
    fi
    if [ "${__deployment}" = "lido_csm" ]; then
      if (whiptail --title "Keys generation" --yesno "Do you want to generate validator keys?" 10 60) then
        query_lido_keys_generation
      else
        lido_keys_attention_message
      fi
      if [ "${NETWORK}" = "holesky" ]; then
        __link="https://csm.testnet.fi"
      else
        __link="https://csm.lido.fi"
      fi
      whiptail --title "Finish" --msgbox "Final steps!\n\n1. Run your node './ethd start'\n\n2. Wait until your node is fully synchronized\n\n4. Open ${__link} to submit your keys with '.eth/validator_keys/deposit-data-*.json' file content\n\n5. Wait for keys validation\n\n6. Import your keys by './ethd keys import'" 19 85
    fi
  else
    unset EXECUTION_CLIENT
    unset GRAFANA_CLIENT

    query_remote_beacon
# This gets used, but shellcheck doesn't recognize that
# shellcheck disable=SC2034
    CL_NODE="${REMOTE_BEACON}"
    query_mev
    query_coinbase
    query_graffiti
  fi

  __during_config=0

  if [ "${__deployment}" = "lido_obol" ]; then
    CL_NODE="http://charon:3600"
    case "${NETWORK}" in
      "mainnet")
# We are using the variable
# shellcheck disable=SC2034
        VE_LOCATOR_ADDRESS="0xC1d0b3DE6792Bf6b4b37EccdcC24e45978Cfd2Eb"
# We are using the variable
# shellcheck disable=SC2034
        VE_ORACLE_ADDRESSES_ALLOWLIST='["0x140Bd8FbDc884f48dA7cb1c09bE8A2fAdfea776E","0xA7410857ABbf75043d61ea54e07D57A6EB6EF186","0x404335BcE530400a5814375E7Ec1FB55fAff3eA2","0x946D3b081ed19173dC83Cd974fC69e1e760B7d78","0x007DE4a5F7bc37E2F26c0cb2E8A95006EE9B89b5","0xEC4BfbAF681eb505B94E4a7849877DC6c600Ca3A","0x61c91ECd902EB56e314bB2D5c5C07785444Ea1c8","0x1Ca0fEC59b86F549e1F1184d97cb47794C8Af58d","0xc79F702202E3A6B0B6310B537E786B9ACAA19BAf"]'
# We are using the variable
# shellcheck disable=SC2034
        VE_STAKING_MODULE_ID="2"
# We are using the variable
# shellcheck disable=SC2034
        LIDO_DV_EXIT_EXIT_EPOCH="194048" # capella
        ;;
      "holesky")
# We are using the variable
# shellcheck disable=SC2034
        VE_LOCATOR_ADDRESS="0x28FAB2059C713A7F9D8c86Db49f9bb0e96Af1ef8"
# We are using the variable
# shellcheck disable=SC2034
        VE_ORACLE_ADDRESSES_ALLOWLIST='["0x12A1D74F8697b9f4F1eEBb0a9d0FB6a751366399","0xD892c09b556b547c80B7d8c8cB8d75bf541B2284","0xf7aE520e99ed3C41180B5E12681d31Aa7302E4e5","0x31fa51343297FFce0CC1E67a50B2D3428057D1b1","0x81E411f1BFDa43493D7994F82fb61A415F6b8Fd4","0x4c75FA734a39f3a21C57e583c1c29942F021C6B7","0xD3b1e36A372Ca250eefF61f90E833Ca070559970","0xF0F23944EfC5A63c53632C571E7377b85d5E6B6f","0xb29dD2f6672C0DFF2d2f173087739A42877A5172","0x3799bDA7B884D33F79CEC926af21160dc47fbe05"]'
# We are using the variable
# shellcheck disable=SC2034
        VE_STAKING_MODULE_ID="2"
# We are using the variable
# shellcheck disable=SC2034
        LIDO_DV_EXIT_EXIT_EPOCH="256" # capella
        ;;
      *)
        ;;
    esac

    if [ -f "./.eth/cluster-lock.json" ]; then
      if (whiptail --title "Lido Obol cluster exists" --yesno "Your cluster has already been created. Continue with it?" 10 60); then
# shellcheck disable=SC2086
        __cluster_lock_is_valid=$(docompose -f ./lido-obol.yml run --rm -v "$(pwd)"/.eth/cluster-lock.json:/cluster-lock.json:ro curl-jq sh -c \
          "cat /cluster-lock.json | jq -r 'all(.cluster_definition.validators[]; .fee_recipient_address == \"'${FEE_RECIPIENT}'\" and .withdrawal_address == \"'$(lido_withdrawal_credentials_address)'\")'" | tail -n 1)
        if [[ "${__cluster_lock_is_valid}" =~ "true" ]]; then
          echo "Your cluster lock is valid."
        else
          whiptail --title "Lido Obol cluster definition" --msgbox "Your cluster lock file './.eth/cluster-lock.json' is not valid.\n\nCheck that every validator has \`fee_recipient_address\` and \`withdrawal_address\` equal to Lido contracts and try again.\n\nLido fee recipient: '${FEE_RECIPIENT}'\nLido withdrawal credentials: '$(lido_withdrawal_credentials_address)'" 14 90
          echo "Your cluster lock is NOT valid."
          exit 1
        fi
      elif (whiptail --title "Lido Obol cluster creation" --yesno "Backup a previously created cluster to create a new one?" 10 80); then
        ${__as_owner} cp -vr ./.eth ./.eth_backup_"$EPOCHSECONDS"
        ${__as_owner} rm -rf ./.eth
        query_lido_obol_enr
        query_lido_obol_cluster_definition
        query_lido_obol_cluster_dkg
      else
        whiptail --title "Lido Obol cluster creation" --msgbox "The \`.eth\` folder must be empty or non-existent to continue" 10 80
        echo "The \`.eth\` folder must be empty to create a new cluster"
        exit 1
      fi
    else
      if [ -f "./.eth/charon-enr-private-key" ] && [ -f "./.eth/charon-enr-public-key" ]; then
        if (whiptail --title "Lido Obol operator ENR creation" --yesno "You already have ENR. Use it?" 8 50); then
          echo "Use existing ENR"
        else
          ${__as_owner} cp -vr ./.eth ./.eth_backup_"$EPOCHSECONDS"
          ${__as_owner} rm -rf ./.eth
          query_lido_obol_enr
        fi
      else
        query_lido_obol_enr
      fi

      if [ -f "./.eth/cluster-definition.json" ]; then
        if (whiptail --title "Lido Obol cluster creation in process" --yesno "You already have cluster definition. Use it?" 10 60); then
# shellcheck disable=SC2086
          __cluster_definition_is_valid=$(docompose -f ./lido-obol.yml run --rm -v "$(pwd)"/.eth/cluster-definition.json:/cluster-definition.json:ro curl-jq sh -c \
            "cat /cluster-definition.json | jq -r 'all(.validators[]; .fee_recipient_address == \"'${FEE_RECIPIENT}'\" and .withdrawal_address == \"'$(lido_withdrawal_credentials_address)'\")'" | tail -n 1)
          if [ "${__cluster_definition_is_valid}" = "true" ]; then
            echo "Your cluster definition is valid."
          else
            whiptail --title "Lido Obol cluster creation" --msgbox "Your cluster definition is not valid.\n\nCheck that every validator has \`fee_recipient_address\` and \`withdrawal_address\` equal to Lido contracts and try again.\n\nLido fee recipient: '${FEE_RECIPIENT}'\nLido withdrawal credentials: '$(lido_withdrawal_credentials_address)'" 14 90
            echo "Your cluster definition is NOT valid."
            exit 1
          fi
        else
          query_lido_obol_cluster_definition
        fi
      else
        query_lido_obol_cluster_definition
      fi
      query_lido_obol_cluster_dkg
    fi

# We are using the variable
# shellcheck disable=SC2034
    VE_OPERATOR_ID=$(whiptail --title "Lido Operator ID" --inputbox "Put your Operator ID from Lido Operators dashboard \
(right-click to paste)" 10 60 3>&1 1>&2 2>&3)
    __obol_prom_remote_token=$(whiptail --title "Obol prometheus" --inputbox "Put Obol Prometheus remote write token \
(right-click to paste)" 10 60 3>&1 1>&2 2>&3)
    cat ./prometheus/obol-prom.yml > ./prometheus/custom-prom.yml
    sed -i'.original' "s|      credentials: OBOL_PROM_REMOTE_WRITE_TOKEN|      credentials: ${__obol_prom_remote_token}|" ./prometheus/custom-prom.yml
    rm -f ./prometheus/custom-prom.yml.original
  fi

  COMPOSE_FILE="${CONSENSUS_CLIENT}"
  if [ -n "${EXECUTION_CLIENT+x}" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:${EXECUTION_CLIENT}"
  fi
  if [[ "${__deployment}" = "ssv" || "${__deployment}" = "lido_ssv" ]]; then
    COMPOSE_FILE="${COMPOSE_FILE}:ssv.yml"
    if [[ -n "${__ssv_operator_id}" && ! "${__ssv_operator_id}" = "-1" ]]; then
      COMPOSE_FILE="${COMPOSE_FILE}:ssv-dkg.yml"
    fi
  fi
  if [ -n "${GRAFANA_CLIENT+x}" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:${GRAFANA_CLIENT}"
  fi
  if [ "${MEV_BOOST}" = "true" ] && [ ! "${__deployment}" = "rocket" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:mev-boost.yml"
  fi
  if [ "${__deployment}" = "lido_obol" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:lido-obol.yml"
  fi
  if { [ "${__deployment}" = "node" ] || [ "${__deployment}" = "rocket" ]; } \
  && [ "${NETWORK}" = "holesky" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:deposit-cli.yml"
  fi
  if [ "${__deployment}" = "lido_csm" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:deposit-cli.yml"
  fi
# Not multi-arch, this would break on ARM64
#    COMPOSE_FILE="${COMPOSE_FILE}:ethdo.yml"
  if [ "${__deployment}" = "rocket" ]; then
    COMPOSE_FILE="${COMPOSE_FILE}:ext-network.yml"
    sed -i'.original' -e "s~name: traefik_default~name: rocketpool_net~" ext-network.yml
  fi

  echo "Your COMPOSE_FILE is:" "${COMPOSE_FILE}"

  var=FEE_RECIPIENT
  set_value_in_env
  var=GRAFFITI
  set_value_in_env
  var=CL_NODE
  set_value_in_env
  var=RAPID_SYNC_URL
  set_value_in_env
  var=COMPOSE_FILE
  set_value_in_env
  var=EL_NODE
  set_value_in_env
  var=JWT_SECRET
  set_value_in_env
  var=NETWORK
  set_value_in_env
  var=MEV_BOOST
  set_value_in_env
  var=MEV_RELAYS
  set_value_in_env
  if [ "${__deployment}" = "lido_obol" ]; then
    var=LIDO_DV_EXIT_EXIT_EPOCH
    set_value_in_env
    var=VE_OPERATOR_ID
    set_value_in_env
    var=VE_LOCATOR_ADDRESS
    set_value_in_env
    var=VE_ORACLE_ADDRESSES_ALLOWLIST
    set_value_in_env
    var=VE_STAKING_MODULE_ID
    set_value_in_env
# We are using the variable
# shellcheck disable=SC2034
    ENABLE_DIST_ATTESTATION_AGGR="true"
    var=ENABLE_DIST_ATTESTATION_AGGR
    set_value_in_env
  fi
  if [[ "${NETWORK}" = "gnosis" ]] && [[ "${CONSENSUS_CLIENT}" =~ "nimbus" ]] ; then
# We are using the variable
# shellcheck disable=SC2034
    NIM_DOCKERFILE=Dockerfile.sourcegnosis
    var=NIM_DOCKERFILE
    set_value_in_env
  fi
  if uname -m | grep -q riscv64; then
# We are using the variable
# shellcheck disable=SC2034
    NIM_DOCKERFILE=Dockerfile.source
    var=NIM_DOCKERFILE
    set_value_in_env
# We are using the variable
# shellcheck disable=SC2034
    GETH_DOCKERFILE=Dockerfile.source
    var=GETH_DOCKERFILE
    set_value_in_env
  fi
  var="SIREN_PASSWORD"
  SIREN_PASSWORD=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  if [ -z "${SIREN_PASSWORD}" ]; then
    SIREN_PASSWORD=$(head -c 8 /dev/urandom | od -A n -t u8 | tr -d '[:space:]' | sha256sum | head -c 32)
    set_value_in_env
  fi

  ${__as_owner} rm .env.original

  pull_and_build
  nag_os_version

  echo
  echo "Your configuration file is: $(dirname "$(realpath "${BASH_SOURCE[0]}")")/${ENV_FILE}"
  echo "You can change advanced config items with \"nano .env\" when in the $(dirname "$(realpath "${BASH_SOURCE[0]}")") directory."
  echo
}


version() {
  grep "^This is" README.md
  echo
  var="COMPOSE_FILE"
  value=$(sed -n -e "s/^${var}=\(.*\)/\1/p" "${ENV_FILE}" || true)
  # Client versions
  case "${value}" in
    *lido-obol.yml* )
      docompose exec charon charon version
      echo
      ;;&
    *ssv.yml* )
      docompose exec ssv-node /go/bin/ssvnode --version
      echo
      ;;&
    *lighthouse.yml* | *lighthouse-cl-only* )
      docompose exec consensus lighthouse --version
      echo
      ;;&
    *lighthouse-vc-only* )
      docompose exec validator lighthouse --version
      echo
      ;;&
    *lodestar.yml* | *lodestar-cl-only* )
      docompose exec consensus node /usr/app/node_modules/.bin/lodestar --version
      echo
      ;;&
    *lodestar-vc-only* )
      docompose exec validator node /usr/app/node_modules/.bin/lodestar --version
      echo
      ;;&
    *prysm.yml* )
      docompose exec consensus beacon-chain --version
      echo
      docompose exec validator validator --version
      echo
      ;;&
    *prysm-cl-only* )
      docompose exec consensus beacon-chain --version
      echo
      ;;&
    *prysm-vc-only* )
      docompose exec validator validator --version
      echo
      ;;&
    *nimbus.yml* | *nimbus-allin1.yml* |  *nimbus-cl-only* )
      docompose exec consensus nimbus_beacon_node --version
      echo
      ;;&
    *nimbus-vc-only* )
      docompose exec validator nimbus_validator_client --version
      echo
      ;;&
    *teku.yml* | *teku-allin1.yml* | *teku-cl-only* )
      docompose exec consensus /opt/teku/bin/teku --version
      echo
      ;;&
    *teku-vc-only* )
      docompose exec validator /opt/teku/bin/teku --version
      echo
      ;;&
    *grandine.yml* | *grandine-allin1.yml* | *grandine-cl-only* )
      docompose exec consensus grandine --version
      echo
      ;;&
    *grandine-vc-only* )
      docompose exec validator grandine --version
      echo
      ;;&
    *geth.yml* )
      docompose exec execution geth version
      echo
      ;;&
    *reth.yml* )
      docompose exec execution reth --version
      echo
      ;;&
    *besu.yml* )
      docompose exec execution /opt/besu/bin/besu --version
      echo
      ;;&
    *nethermind.yml* )
      docompose exec execution /nethermind/nethermind --version
      echo
      ;;&
    *erigon.yml* )
      docompose exec execution erigon --version
      echo
      ;;&
    *web3signer.yml* )
      docompose exec web3signer /opt/web3signer/bin/web3signer --version
      echo
      docompose exec postgres pg_config --version
      echo
      ;;&
    *mev-boost.yml* )
      docompose exec mev-boost /app/mev-boost -version
      echo
      ;;&
    *grafana.yml* )
      docompose exec prometheus /bin/prometheus --version
      echo
      echo -n "Grafana "
      docompose exec grafana /run.sh -v
      echo
      ;;&
    *traefik-*.yml* )
      echo "Traefik"
      docompose exec traefik traefik version
      echo
      ;;&
  esac
}


__update_help() {
  echo "usage: $__me update [--refresh-targets] [--non-interactive]"
  echo
  echo "Updates Eth Docker itself, as required the contents of \".env\", and the clients."
  echo
  echo "A combination of \"git pull\" for Eth Docker, some bash scripting to bring new variables from \"default.env\","
  echo "and \"docker compose pull\" as well as \"docker compose build\" for the clients."
  echo
  echo "If warranted, will also offer resync when clients require it, or upgrade of PostgreSQL version."
  echo
  echo "\"--refresh-targets\" sets Docker tags, source targets, and repos of clients back to the defaults in \"default.env\"."
  echo "\"--non-interactive\" does not ask questions and assumes Yes for database resyncs and migrations."
  echo
}


__full_help() {
  echo "usage: $__me [-h|--help] <command>"
  echo
  echo "commands:"
  echo "  install"
  echo "    attempts to install Docker and Docker Compose for you"
  echo "  config"
  echo "    configures ${__project_name} with your choice of Ethereum clients"
  echo "  keys ACTION [--non-interactive]"
  echo "    list, delete, import keys; their fee recipients; and gas fees"
  echo "    Run without ACTION to get help text"
  echo "  update [--refresh-targets] [--non-interactive]"
  echo "    updates all client versions and ${__project_name} itself"
  echo "    --refresh-targets will reset your custom build targets in ${ENV_FILE} to defaults"
  echo "  up|start [service-name]"
  echo "    starts the Ethereum node, or restarts containers that had their image or"
  echo "    configuration changed. Can also start a specific service by name"
  echo "  down|stop [service-name]"
  echo "    stops the Ethereum node, or a specific service by name"
  echo "  restart [service-name]"
  echo "    restarts the Ethereum node, or a specific service by name, a combination of down and up"
  echo "  version"
  echo "    prints the version(s) of currently running client(s)"
  echo "  logs"
  echo "    shows logs"
  echo "  cmd <compose-command>"
  echo "    executes an arbitrary Docker Compose command. Use \"cmd help\" to list them"
  echo "  terminate"
  echo "    stops the Ethereum node and destroys all data stores"
  echo "  prune-nethermind [--non-interactive]"
  echo "    restarts the Nethermind execution client and prunes its DB."
  echo "  prune-besu [--non-interactive]"
  echo "    stops the Besu execution client and prunes trie-logs."
  echo "  prune-reth [--non-interactive]"
  echo "    stops the Reth execution client and prunes its DB."
  echo "  prune-lighthouse [--non-interactive]"
  echo "    stops the Lighthouse consensus client and prunes state."
  echo "  resync-execution"
  echo "    removes the execution layer database and forces a resync."
  echo "  resync-consensus"
  echo "    removes the consensus layer database and forces a resync."
  echo "  space"
  echo "    show Docker volume space usage"
  echo "  attach-geth"
  echo "    launches an interactive geth attach repl"
  echo "  help"
  echo "    print this help screen"
  echo
  echo "  Instead of \"--non-interactive\" you may also use the \"ETHD_FRONTEND=noninteractive\" environment variable"
  echo
  echo "The logs command can be appended by flags and specify the container(s). Example: "
  echo
  echo "  $__me logs -f --tail 50 execution"
  echo "    shows logs only for execution service"
  echo
  echo " Give feedback and report issues on GitHub:"
  echo "  * https://github.com/eth-educators/eth-docker"
  echo " Get support on Discord:"
  echo "  * http://discord.gg/ethstaker"
}


help() {
  case $* in
    *update*) __update_help;;
    *) __full_help;;
  esac
}

# Main body from here
ENV_FILE=.env
__during_config=0
__during_update=0
__during_postgres=0
__during_migrate=0
__migrated=0
__command=""
__me=$(basename "${BASH_SOURCE[0]}")
if [ ! -f ~/.profile ] || ! grep -q "alias ethd" ~/.profile; then
  __me="./$__me"
fi

trap 'handle_error $? $LINENO' ERR

if [[ "$#" -eq 0 || "$*" =~ "-h" ]]; then # Lazy match for -h and --help but also --histogram, so careful here
  help "$@"
  exit 0
fi

cd "$(dirname "$(realpath "${BASH_SOURCE[0]}")")"
# Use this to make sure root doesn't end up owning files
# shellcheck disable=SC2012
OWNER=$(ls -ld . | awk '{print $3}')
OWNER_GROUP=$(id -gn "${OWNER}")

if [ "${OWNER}" == "root" ]; then
  echo "Please install ${__project_name} as a non-root user."
  exit 0
fi

__command="$1"
shift
__params=$*

handle_root
determine_distro
prep_conffiles

check_for_snap

# Don't check for Docker before it's installed
if [ "$__command" = "install" ]; then
  $__command "$@"
  exit "$?"
fi

handle_docker_sudo
check_compose_version

if [ "${__old_compose}" -eq 1 ]; then
  echo
  echo "You are using docker-compose ${__compose_version}, which is unsupported by Docker, Inc."
  echo "${__project_name} only supports Compose V2."
  echo
  echo "You can install it with \"sudo apt update && sudo apt install docker-compose-v2\"."
  echo "You can remove the old docker-compose:"
  echo "\"sudo apt-mark manual docker.io && sudo apt --autoremove remove docker-compose\"."
  exit 0
fi

if [ "${__old_docker}" -eq 1 ]; then
  echo
  echo "Docker version ${__docker_version} detected. This version is no longer supported."
  echo "Please update to a current version. Supported versions can be seen at https://endoflife.date/docker-engine."
  echo
  echo "This should be as simple as \"sudo apt update && sudo apt dist-upgrade\" on Debian/Ubuntu"
  echo "or updating Docker Desktop on macOS and Windows."
  exit 0
fi

if ! type -P whiptail >/dev/null 2>&1; then
  echo "Please install the package whiptail or newt before running ${__project_name}."
  exit 0
fi

if ! dodocker images >/dev/null 2>&1; then
  echo "Please ensure you can call $__docker_exe before running ${__project_name}."
  exit 0
fi

if ! docompose --help >/dev/null 2>&1; then
  echo "Please ensure you can call $__compose_exe before running ${__project_name}."
  exit 0
fi

case "$__command" in
  help|config|keys|update|up|start|down|stop|restart|version|logs|cmd|terminate|prune-nethermind\
      |prune-besu|prune-reth|prune-lighthouse|resync-execution|resync-consensus|attach-geth|keyimport|space)
    $__command "$@";;
  *)
    echo "Unrecognized command $__command"
    help
    ;;
esac

check_disk_space

if [ "${__compose_upgraded}" -eq 1 ]; then
  echo
  echo "You updated Docker Compose to V2."
  echo "The \"docker-compose\" command is gone and replaced with \"docker compose\"."
  echo
  echo "You can create an alias for \"docker-compose\" by adding this line to your \"~/.profile\":"
  echo "alias docker-compose=\"docker compose\""
  echo
  echo "Optionally, you can switch to docker-ce."
  echo "Please see https://ethdocker.com/Usage/Prerequisites#switching-from-dockerio-to-docker-ce for instructions."
fi
